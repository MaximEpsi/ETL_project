{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explication des cellules d'initialisation\n",
    "Nous avons testé plusieurs approches : en local, avec Pycharm. Dans le cloud, avec Kaggle & Google Colab.\n",
    "Nous avons donc mis en place plusieurs configurations pour chaque environnement.\n",
    "Si local, vérifier que java 11 est bien installé. Si Kaggle, uploader le csv comme dataset afin de l'avoir à disposition.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install pyspark\n",
    "file_path_csv = \"/kaggle/input/openfoodfacts/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"/kaggle/working/en.openfoodfacts.org.products.parquet\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path_csv = \"./data/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"./data/en.openfoodfacts.org.products.parquet\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start_time = time.time()\n",
    "print(\"Démarrage du script...\")\n",
    "\n",
    "# Initialiser une SparkSession avec des logs réduits\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exploration OpenFoodFacts\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Réduction des logs\n",
    "\n",
    "print(\"PySpark chargé\")\n",
    "\n",
    "try:\n",
    "    # Charger le fichier CSV en tant que DataFrame Spark puis échantillonne 20%\n",
    "    df_csv_before_sample = spark.read.csv(file_path_csv, header=True, inferSchema=True, sep=\"\\t\")\n",
    "    print(\"Fichier CSV chargé.\")\n",
    "    df_csv = df_csv_before_sample.sample(withReplacement=False, fraction=0.2)  # Échantillonnage à 20%\n",
    "    print(\"Echantillonage terminé\")\n",
    "\n",
    "    # Sauvegarder le DataFrame au format Parquet\n",
    "    df_csv.write.parquet(file_path_parquet, mode=\"overwrite\")\n",
    "    print(\"Données sauvegardées au format Parquet.\")\n",
    "\n",
    "    # Charger le fichier Parquet pour une analyse future\n",
    "    df_parquet = spark.read.parquet(file_path_parquet)\n",
    "    print(\"Fichier Parquet chargé.\")\n",
    "\n",
    "    # Création de la table Hive\n",
    "    print(\"Création et insertion dans la table Hive...\")\n",
    "    hive_table_start_time = time.time()\n",
    "\n",
    "    # Écrire les données dans la table Hive\n",
    "    df_csv.write.mode(\"overwrite\").saveAsTable(\"hive_table\")\n",
    "    \n",
    "except:\n",
    "    print(\"ERRRRROOOOOOR\")\n",
    "\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Temps d'exécution : {elapsed_time:.2f} secondes\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mesure du temps pour compter les lignes du DataFrame CSV\n",
    "csv_start_time = time.time()\n",
    "csv_row_count = df_csv.count()  # Compter les lignes\n",
    "csv_end_time = time.time()\n",
    "csv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\n",
    "print(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes du DataFrame Parquet\n",
    "parquet_start_time = time.time()\n",
    "parquet_row_count = df_parquet.count()  # Compter les lignes\n",
    "parquet_end_time = time.time()\n",
    "parquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\n",
    "print(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes de la table Hive\n",
    "hive_start_time = time.time()\n",
    "df_hive = spark.sql(\"SELECT * FROM hive_table\")  # Charger la table Hive\n",
    "hive_row_count = df_hive.count()  # Compter les lignes\n",
    "hive_end_time = time.time()\n",
    "hive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\n",
    "print(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n",
    "\n",
    "# Comparaison des temps\n",
    "print(\"\\nComparaison des performances :\")\n",
    "print(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse préliminaire\n",
    "### 1. Mise en valeur des lignes, colonnes\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mesure du temps pour compter les lignes du DataFrame CSV\n",
    "csv_start_time = time.time()\n",
    "csv_row_count = df_csv.count()\n",
    "csv_end_time = time.time()\n",
    "csv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\n",
    "print(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes du DataFrame Parquet\n",
    "parquet_start_time = time.time()\n",
    "parquet_row_count = df_parquet.count()\n",
    "parquet_end_time = time.time()\n",
    "parquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\n",
    "print(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes de la table Hive\n",
    "hive_start_time = time.time()\n",
    "df_hive = spark.sql(\"SELECT * FROM hive_table\")\n",
    "hive_row_count = df_hive.count()\n",
    "hive_end_time = time.time()\n",
    "hive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\n",
    "print(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n",
    "\n",
    "# Comparaison des temps\n",
    "print(\"\\nComparaison des performances :\")\n",
    "print(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### .2 Gestion des valeurs manquantes\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate missing data percentage for each column\n",
    "total_rows = df_parquet.count()\n",
    "missing_data = (\n",
    "    df_parquet.select([\n",
    "        (count(when(col(c).isNull() | (col(c) == \"\"), c)) / total_rows).alias(c)\n",
    "        for c in df_parquet.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Transform columns into rows (melt operation)\n",
    "missing_data_melted = missing_data.selectExpr(\n",
    "    \"stack({0}, {1}) as (Column, MissingPercentage)\".format(\n",
    "        len(df_parquet.columns),\n",
    "        \", \".join([f\"'{col}', `{col}`\" for col in df_parquet.columns])\n",
    "    )\n",
    ").filter(col(\"MissingPercentage\").isNotNull()).orderBy(col(\"MissingPercentage\").desc())\n",
    "\n",
    "# Identify columns with 100% missing data\n",
    "columns_to_drop = (\n",
    "    missing_data_melted.filter(col(\"MissingPercentage\") == 1.0)\n",
    "    .select(\"Column\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Drop columns with 100% missing values\n",
    "df_cleanedby_missing_value = df_parquet.drop(*columns_to_drop)\n",
    "\n",
    "# Display the top 10 columns with the highest missing percentages\n",
    "print(\"Top 10 columns with the highest missing percentages:\")\n",
    "missing_data_melted.show(10, truncate=False)\n",
    "\n",
    "# Print dropped columns\n",
    "print(f\"Columns dropped due to 100% missing values: {columns_to_drop}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Gestion des valeurs doublons"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Analyzing Duplicates in 'code', 'product_name', and 'brands'\n",
    "duplicates = (\n",
    "    df_cleanedby_missing_value.groupBy(\"code\", \"product_name\", \"brands\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "# Affiche le nombre de doublons identifiés\n",
    "print(f\"There are {duplicates.count()} duplicate rows based on 'code', 'product_name', and 'brands'.\")\n",
    "duplicates.show(truncate=False)\n",
    "\n",
    "# Remove duplicates where 'code', 'product_name', and 'brands' are the same\n",
    "df_cleaned_by_duplicate = df_cleanedby_missing_value.dropDuplicates([\"code\", \"product_name\", \"brands\"])\n",
    "\n",
    "print(f\"Number of rows before removing duplicates: {df_cleanedby_missing_value.count()}\")\n",
    "print(f\"Number of rows after removing duplicates: {df_cleaned_by_duplicate.count()}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Handle outliers"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType\n",
    "\n",
    "# Extraire les valeurs numériques de la colonne \"quantity\"\n",
    "df_cleaned_by_outliers = df_cleaned_by_duplicate.withColumn(\n",
    "    \"quantity_numeric\",\n",
    "    regexp_extract(col(\"quantity\"), r\"(\\d+)\", 1).cast(\"double\")\n",
    ")\n",
    "\n",
    "# Détecter les colonnes numériques\n",
    "numeric_columns = [\n",
    "    field.name for field in df_cleaned_by_outliers.schema.fields\n",
    "    if isinstance(field.dataType, (IntegerType, DoubleType, FloatType))\n",
    "]\n",
    "print(f\"Numeric columns detected: {numeric_columns}\")\n",
    "\n",
    "if not numeric_columns:\n",
    "    print(\"No numeric columns found. Please check your data.\")\n",
    "else:\n",
    "    total_rows_before = df_cleaned_by_outliers.count()\n",
    "    removed_rows_total = 0\n",
    "\n",
    "    # Boucle sur les colonnes numériques pour détecter les outliers\n",
    "    for column in numeric_columns:\n",
    "        try:\n",
    "            # Calcul des quartiles avec approxQuantile\n",
    "            quantiles = df_cleaned_by_outliers.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "            if len(quantiles) < 2:\n",
    "                print(f\"Column '{column}' has insufficient data. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            q1, q3 = quantiles\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "            # Filtrer les outliers\n",
    "            df_outliers = df_cleaned_by_outliers.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "            removed_rows = df_outliers.count()\n",
    "            removed_rows_total += removed_rows\n",
    "\n",
    "            print(f\"Column '{column}': {removed_rows} rows detected as outliers.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column '{column}': {e}\")\n",
    "\n",
    "    print(f\"Total rows before filtering: {total_rows_before}\")\n",
    "    print(f\"Total rows removed as outliers: {removed_rows_total}\")\n",
    "    print(f\"Total rows remaining: {total_rows_before - removed_rows_total}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data cleaning\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T11:24:45.910907Z",
     "start_time": "2024-12-03T11:24:45.017367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# display the schema of the cleaned DataFrame\n",
    "df_parquet = df_outliers\n",
    "df_parquet.describe()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_outliers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# display the schema of the cleaned DataFrame\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_parquet \u001B[38;5;241m=\u001B[39m \u001B[43mdf_outliers\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_outliers' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#select columns to keep\n",
    "selected_column = [\n",
    "    'code',\n",
    "    'product_name',\n",
    "    'brands',\n",
    "    'categories',\n",
    "    \"main_category\",\n",
    "    'quantity',\n",
    "    'packaging',\n",
    "    'countries',\n",
    "    'ingredients_text',\n",
    "    'allergens',\n",
    "    'serving_size',\n",
    "    'energy-kcal_100g',\n",
    "    'fat_100g',\n",
    "    'saturated-fat_100g',\n",
    "    \"proteins_100g\",\n",
    "    'sugars_100g',\n",
    "    'salt_100g',\n",
    "    'nutriscore_score',\n",
    "    'nutriscore_grade',\n",
    "    \"food_groups_en\",\n",
    "    \"sodium\",\n",
    "    \"sugar\",\n",
    "    \"fiber\"\n",
    "]\n",
    "\n",
    "df_transformed = df_parquet.select(selected_column)\n",
    "df_transformed.show(5, truncate=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert the columns to the appropriate format\n",
    "column_to_convert = [\"quantity\", \"nutriscore_score\", \"energy-kcal_100g\",\n",
    "                     \"fat_100g\", \"saturated-fat_100g\", \"proteins_100g\", \"sugars_100g\", \"salt_100g\"]\n",
    "# apply the conversion\n",
    "for column in column_to_convert:\n",
    "    df_transformed = df_transformed.withColumn(column, col(column).cast(\"double\"))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert code in string\n",
    "df_transformed = df_transformed.withColumn(\"code\", col(\"code\").cast(\"string\"))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformation des données Transform :\n",
    "Ajouter des colonnes calculées, par exemple : Indice de qualité nutritionnelle \n",
    "Calculer un score basé sur les nutriments (e.g., sodium, sugar, fiber). \n",
    "Extraire la catégorie principale d'un produit (e.g., \"boissons\", \"snacks\"). \n",
    "Regrouper les données par catégories (categories) pour analyser les tendances (e.g., moyenne des calories par catégorie).\n",
    "\n",
    "--> Quel calcules effectuer ?  \n",
    "--> Quel catégories créer ?\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Transformation\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse exploratoire :\n",
    "Utiliser des fonctions de calcul sur fenêtre pour : \n",
    "Trouver les produits les plus caloriques par catégorie. \n",
    "Identifier les tendances de production par brands (marques). \n",
    "Générer des statistiques descriptives (e.g., médiane, moyenne des nutriments par catégorie"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Exploration\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sauvegarde des données Save :\n",
    "Partitionner les données par catégories (categories) et années (year). \n",
    "Sauvegarder les résultats transformés en format Parquet avec compression Snappy. \n",
    "Sauvegarder les résultats transformés dans les bases de données: postgresql/sqlserver/mysql/Snowflake/BigQuery"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Sauvegarde des données (load)\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Présentation des résultats :\n",
    "Visualiser les résultats sous forme de graphiques ou tableaux \n",
    "(les étudiants peuvent utiliser un outil comme Jupyter Notebook en local ou Google Colab "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Présentation des données\")"
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6107906,
     "sourceId": 9935398,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
