{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explication des cellules d'initialisation\n",
    "Nous avons testé plusieurs approches : en local, avec Pycharm. Dans le cloud, avec Kaggle & Google Colab.\n",
    "Nous avons donc mis en place plusieurs configurations pour chaque environnement.\n",
    "Si local, vérifier que java 11 est bien installé. Si Kaggle, uploader le csv comme dataset afin de l'avoir à disposition.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T10:29:08.996141Z",
     "start_time": "2024-12-03T10:29:08.822047Z"
    }
   },
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "file_path_csv = \"/kaggle/input/openfoodfacts/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"/kaggle/working/en.openfoodfacts.org.products.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T10:29:17.789399Z",
     "start_time": "2024-12-03T10:29:17.786574Z"
    }
   },
   "source": [
    "file_path_csv = \"./data/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"./data/en.openfoodfacts.org.products.parquet\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T10:30:40.540323Z",
     "start_time": "2024-12-03T10:29:22.335944Z"
    }
   },
   "source": [
    "start_time = time.time()\n",
    "print(\"Démarrage du script...\")\n",
    "\n",
    "# Initialiser une SparkSession avec des logs réduits\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exploration OpenFoodFacts\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Réduction des logs\n",
    "\n",
    "print(\"PySpark chargé\")\n",
    "\n",
    "try:\n",
    "    # Charger le fichier CSV en tant que DataFrame Spark puis échantillonne 20%\n",
    "    df_csv_before_sample = spark.read.csv(file_path_csv, header=True, inferSchema=True, sep=\"\\t\")\n",
    "    print(\"Fichier CSV chargé.\")\n",
    "    df_csv = df_csv_before_sample.sample(withReplacement=False, fraction=0.2)  # Échantillonnage à 20%\n",
    "    print(\"Echantillonage terminé\")\n",
    "\n",
    "    # Sauvegarder le DataFrame au format Parquet\n",
    "    df_csv.write.parquet(file_path_parquet, mode=\"overwrite\")\n",
    "    print(\"Données sauvegardées au format Parquet.\")\n",
    "\n",
    "    # Charger le fichier Parquet pour une analyse future\n",
    "    df_parquet = spark.read.parquet(file_path_parquet)\n",
    "    print(\"Fichier Parquet chargé.\")\n",
    "\n",
    "    # Création de la table Hive\n",
    "    print(\"Création et insertion dans la table Hive...\")\n",
    "    hive_table_start_time = time.time()\n",
    "\n",
    "    # Écrire les données dans la table Hive\n",
    "    df_csv.write.mode(\"overwrite\").saveAsTable(\"hive_table\")\n",
    "    \n",
    "except:\n",
    "    print(\"ERRRRROOOOOOR\")\n",
    "\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Temps d'exécution : {elapsed_time:.2f} secondes\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage du script...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 11:29:23 WARN Utils: Your hostname, cedric-galaxy resolves to a loopback address: 127.0.1.1; using 192.168.1.26 instead (on interface wlo1)\n",
      "24/12/03 11:29:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/03 11:29:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark chargé\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier CSV chargé.\n",
      "Echantillonage terminé\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données sauvegardées au format Parquet.\n",
      "Fichier Parquet chargé.\n",
      "Création et insertion dans la table Hive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================================>      (67 + 9) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution : 78.20 secondes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T16:05:59.018727Z",
     "iopub.status.busy": "2024-12-02T16:05:59.018167Z",
     "iopub.status.idle": "2024-12-02T16:06:20.319739Z",
     "shell.execute_reply": "2024-12-02T16:06:20.318332Z",
     "shell.execute_reply.started": "2024-12-02T16:05:59.018674Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-03T10:35:35.393502Z",
     "start_time": "2024-12-03T10:35:28.125350Z"
    }
   },
   "source": [
    "# Mesure du temps pour compter les lignes du DataFrame CSV\n",
    "csv_start_time = time.time()\n",
    "csv_row_count = df_csv.count()  # Compter les lignes\n",
    "csv_end_time = time.time()\n",
    "csv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\n",
    "print(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes du DataFrame Parquet\n",
    "parquet_start_time = time.time()\n",
    "parquet_row_count = df_parquet.count()  # Compter les lignes\n",
    "parquet_end_time = time.time()\n",
    "parquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\n",
    "print(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes de la table Hive\n",
    "hive_start_time = time.time()\n",
    "df_hive = spark.sql(\"SELECT * FROM hive_table\")  # Charger la table Hive\n",
    "hive_row_count = df_hive.count()  # Compter les lignes\n",
    "hive_end_time = time.time()\n",
    "hive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\n",
    "print(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n",
    "\n",
    "# Comparaison des temps\n",
    "print(\"\\nComparaison des performances :\")\n",
    "print(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV (comptage des lignes): 6113.584 ms - Nombre de lignes: 702698\n",
      "Parquet (comptage des lignes): 569.154 ms - Nombre de lignes: 702698\n",
      "Hive (comptage des lignes): 577.465 ms - Nombre de lignes: 702698\n",
      "\n",
      "Comparaison des performances :\n",
      "CSV Execution Time: 6113.584 ms\n",
      "Parquet Execution Time: 569.154 ms\n",
      "Hive Execution Time: 577.465 ms\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse préliminaire\n",
    "### 1. Mise en valeur des lignes, colonnes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T16:11:02.604345Z",
     "iopub.status.busy": "2024-12-02T16:11:02.603921Z",
     "iopub.status.idle": "2024-12-02T16:11:17.785795Z",
     "shell.execute_reply": "2024-12-02T16:11:17.784529Z",
     "shell.execute_reply.started": "2024-12-02T16:11:02.604308Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-03T10:38:16.971741Z",
     "start_time": "2024-12-03T10:38:12.325074Z"
    }
   },
   "source": [
    "# Mesure du temps pour compter les lignes du DataFrame CSV\n",
    "csv_start_time = time.time()\n",
    "csv_row_count = df_csv.count()\n",
    "csv_end_time = time.time()\n",
    "csv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\n",
    "print(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes du DataFrame Parquet\n",
    "parquet_start_time = time.time()\n",
    "parquet_row_count = df_parquet.count()\n",
    "parquet_end_time = time.time()\n",
    "parquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\n",
    "print(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes de la table Hive\n",
    "hive_start_time = time.time()\n",
    "df_hive = spark.sql(\"SELECT * FROM hive_table\")\n",
    "hive_row_count = df_hive.count()\n",
    "hive_end_time = time.time()\n",
    "hive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\n",
    "print(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n",
    "\n",
    "# Comparaison des temps\n",
    "print(\"\\nComparaison des performances :\")\n",
    "print(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV (comptage des lignes): 4385.785 ms - Nombre de lignes: 702698\n",
      "Parquet (comptage des lignes): 117.743 ms - Nombre de lignes: 702698\n",
      "Hive (comptage des lignes): 137.143 ms - Nombre de lignes: 702698\n",
      "\n",
      "Comparaison des performances :\n",
      "CSV Execution Time: 4385.785 ms\n",
      "Parquet Execution Time: 117.743 ms\n",
      "Hive Execution Time: 137.143 ms\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### .2 Gestion des valeurs manquantes\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:43:13.150037Z",
     "iopub.status.busy": "2024-12-02T15:43:13.149548Z",
     "iopub.status.idle": "2024-12-02T15:44:54.449482Z",
     "shell.execute_reply": "2024-12-02T15:44:54.448127Z",
     "shell.execute_reply.started": "2024-12-02T15:43:13.149982Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-03T11:08:20.975810Z",
     "start_time": "2024-12-03T11:08:05.014829Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate missing data percentage for each column\n",
    "total_rows = df_parquet.count()\n",
    "missing_data = (\n",
    "    df_parquet.select([\n",
    "        (count(when(col(c).isNull() | (col(c) == \"\"), c)) / total_rows).alias(c)\n",
    "        for c in df_parquet.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Transform columns into rows (melt operation)\n",
    "missing_data_melted = missing_data.selectExpr(\n",
    "    \"stack({0}, {1}) as (Column, MissingPercentage)\".format(\n",
    "        len(df_parquet.columns),\n",
    "        \", \".join([f\"'{col}', `{col}`\" for col in df_parquet.columns])\n",
    "    )\n",
    ").filter(col(\"MissingPercentage\").isNotNull()).orderBy(col(\"MissingPercentage\").desc())\n",
    "\n",
    "# Identify columns with 100% missing data\n",
    "columns_to_drop = (\n",
    "    missing_data_melted.filter(col(\"MissingPercentage\") == 1.0)\n",
    "    .select(\"Column\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Drop columns with 100% missing values\n",
    "df_cleanedby_missing_value = df_parquet.drop(*columns_to_drop)\n",
    "\n",
    "# Display the top 10 columns with the highest missing percentages\n",
    "print(\"Top 10 columns with the highest missing percentages:\")\n",
    "missing_data_melted.show(10, truncate=False)\n",
    "\n",
    "# Print dropped columns\n",
    "print(f\"Columns dropped due to 100% missing values: {columns_to_drop}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 columns with the highest missing percentages:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 729:====================>                                  (6 + 10) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------+\n",
      "|Column                          |MissingPercentage |\n",
      "+--------------------------------+------------------+\n",
      "|cities                          |1.0               |\n",
      "|allergens_en                    |1.0               |\n",
      "|additives                       |0.9999985769135532|\n",
      "|nutrition-score-uk_100g         |0.9999985769135532|\n",
      "|nervonic-acid_100g              |0.9999971538271064|\n",
      "|elaidic-acid_100g               |0.9999971538271064|\n",
      "|chlorophyl_100g                 |0.9999971538271064|\n",
      "|water-hardness_100g             |0.9999971538271064|\n",
      "|gamma-linolenic-acid_100g       |0.9999957307406596|\n",
      "|dihomo-gamma-linolenic-acid_100g|0.9999957307406596|\n",
      "+--------------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Columns dropped due to 100% missing values: ['cities', 'allergens_en']\n",
      "Execution completed in 15.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3. Gestion des valeurs doublons"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:44:54.460788Z",
     "iopub.status.busy": "2024-12-02T15:44:54.460345Z",
     "iopub.status.idle": "2024-12-02T15:45:10.159777Z",
     "shell.execute_reply": "2024-12-02T15:45:10.157123Z",
     "shell.execute_reply.started": "2024-12-02T15:44:54.460741Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-03T11:12:57.842361Z",
     "start_time": "2024-12-03T11:12:55.507229Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Analyzing Duplicates in 'code', 'product_name', and 'brands'\n",
    "duplicates = (\n",
    "    df_cleanedby_missing_value.groupBy(\"code\", \"product_name\", \"brands\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "# Affiche le nombre de doublons identifiés\n",
    "print(f\"There are {duplicates.count()} duplicate rows based on 'code', 'product_name', and 'brands'.\")\n",
    "duplicates.show(truncate=False)\n",
    "\n",
    "# Remove duplicates where 'code', 'product_name', and 'brands' are the same\n",
    "df_cleaned_by_duplicate = df_cleanedby_missing_value.dropDuplicates([\"code\", \"product_name\", \"brands\"])\n",
    "\n",
    "print(f\"Number of rows before removing duplicates: {df_cleanedby_missing_value.count()}\")\n",
    "print(f\"Number of rows after removing duplicates: {df_cleaned_by_duplicate.count()}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 87 duplicate rows based on 'code', 'product_name', and 'brands'.\n",
      "+---------------------+-----------------------------------------------------+----------------------------------------+-----+\n",
      "|code                 |product_name                                         |brands                                  |count|\n",
      "+---------------------+-----------------------------------------------------+----------------------------------------+-----+\n",
      "|3.250393046759E12    |Porc mariné longue conservation                      |Jean roze                               |2    |\n",
      "|3.25456643534E12     |ERREUR_IMAGES                                        |Pierre Chanau,Auchan                    |2    |\n",
      "|3.25456649037E12     |Saucisses de volaille                                |Auchan                                  |2    |\n",
      "|3.596710344819E12    |Rôti de Porc cuit supérieur                          |Auchan                                  |2    |\n",
      "|3.596710408924E12    |Chiffonnade de jambon cuit                           |Auchan                                  |2    |\n",
      "|3.661273099503E12    |Bananes 5 doigts Cavendish Cat. 1                    |Premier Prix                            |2    |\n",
      "|8.594071424626E12    |Tartare saumon                                       |Carrefour                               |2    |\n",
      "|2.3327665001305117E22|Riz long parfumé                                     |Riz du monde                            |2    |\n",
      "|3.250391535224E12    |Paupiettes de porc                                   |Jean roze                               |2    |\n",
      "|3.560070778669E12    |Cocktail fruit and nuts                              |Carrefour                               |2    |\n",
      "|3.560071424343E12    |Oh juice (Pur Jus Multifruits🍍🍌🍊) & Eau de Coco🥥)|Carrefour                               |2    |\n",
      "|2.03050868536E12     |Baguette a la farine aveyron                         |Auchan                                  |2    |\n",
      "|3.596710170111E12    |Rik & Rok - Mini goûters saveur chocolat             |Rik & Rok,Auchan                        |2    |\n",
      "|5.8612990221900134E17|Finísimo de pechuga de pavo                          |Bonarea                                 |2    |\n",
      "|1.2211917E7          |alphabet                                             |netto                                   |2    |\n",
      "|3.417960015451E12    |Herbes de Provence                                   |Cook,Arcadie                            |2    |\n",
      "|3.523680307797E12    |4 Micro Donuts Choco                                 |Carrefour                               |2    |\n",
      "|3.017800199428E12    |Flageolets bio, 265g                                 |D'Aucy                                  |2    |\n",
      "|8.012666504121E12    |Offelle di parona                                    |Terre d'Italia, Terre ditalia, Carrefour|2    |\n",
      "|8.012666011476E12    |NULL                                                 |Carrefour                               |2    |\n",
      "+---------------------+-----------------------------------------------------+----------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows before removing duplicates: 702698\n",
      "Number of rows after removing duplicates: 702611\n",
      "Execution completed in 2.33 seconds\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.172010Z",
     "iopub.status.busy": "2024-12-02T15:45:10.171558Z",
     "iopub.status.idle": "2024-12-02T15:45:10.259967Z",
     "shell.execute_reply": "2024-12-02T15:45:10.258603Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.171958Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-03T11:11:00.783427Z",
     "start_time": "2024-12-03T11:08:44.131976Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType\n",
    "\n",
    "# Extraire les valeurs numériques de la colonne \"quantity\"\n",
    "df_cleaned_by_outliers = df_cleaned_by_duplicate.withColumn(\n",
    "    \"quantity_numeric\",\n",
    "    regexp_extract(col(\"quantity\"), r\"(\\d+)\", 1).cast(\"double\")\n",
    ")\n",
    "\n",
    "# Détecter les colonnes numériques\n",
    "numeric_columns = [\n",
    "    field.name for field in df_cleaned_by_outliers.schema.fields\n",
    "    if isinstance(field.dataType, (IntegerType, DoubleType, FloatType))\n",
    "]\n",
    "print(f\"Numeric columns detected: {numeric_columns}\")\n",
    "\n",
    "if not numeric_columns:\n",
    "    print(\"No numeric columns found. Please check your data.\")\n",
    "else:\n",
    "    total_rows_before = df_cleaned_by_outliers.count()\n",
    "    removed_rows_total = 0\n",
    "\n",
    "    # Boucle sur les colonnes numériques pour détecter les outliers\n",
    "    for column in numeric_columns:\n",
    "        try:\n",
    "            # Calcul des quartiles avec approxQuantile\n",
    "            quantiles = df_cleaned_by_outliers.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "            if len(quantiles) < 2:\n",
    "                print(f\"Column '{column}' has insufficient data. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            q1, q3 = quantiles\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "            # Filtrer les outliers\n",
    "            df_outliers = df_cleaned_by_outliers.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "            removed_rows = df_outliers.count()\n",
    "            removed_rows_total += removed_rows\n",
    "\n",
    "            print(f\"Column '{column}': {removed_rows} rows detected as outliers.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column '{column}': {e}\")\n",
    "\n",
    "    print(f\"Total rows before filtering: {total_rows_before}\")\n",
    "    print(f\"Total rows removed as outliers: {removed_rows_total}\")\n",
    "    print(f\"Total rows remaining: {total_rows_before - removed_rows_total}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns detected: ['code', 'created_t', 'last_modified_t', 'last_updated_t', 'serving_quantity', 'additives_n', 'nutriscore_score', 'nova_group', 'ecoscore_score', 'product_quantity', 'unique_scans_n', 'completeness', 'last_image_t', 'energy-kj_100g', 'energy-kcal_100g', 'energy_100g', 'energy-from-fat_100g', 'fat_100g', 'saturated-fat_100g', 'butyric-acid_100g', 'caproic-acid_100g', 'caprylic-acid_100g', 'capric-acid_100g', 'lauric-acid_100g', 'myristic-acid_100g', 'palmitic-acid_100g', 'stearic-acid_100g', 'arachidic-acid_100g', 'behenic-acid_100g', 'lignoceric-acid_100g', 'cerotic-acid_100g', 'montanic-acid_100g', 'melissic-acid_100g', 'unsaturated-fat_100g', 'monounsaturated-fat_100g', 'omega-9-fat_100g', 'polyunsaturated-fat_100g', 'omega-3-fat_100g', 'omega-6-fat_100g', 'alpha-linolenic-acid_100g', 'eicosapentaenoic-acid_100g', 'docosahexaenoic-acid_100g', 'linoleic-acid_100g', 'arachidonic-acid_100g', 'gamma-linolenic-acid_100g', 'dihomo-gamma-linolenic-acid_100g', 'oleic-acid_100g', 'elaidic-acid_100g', 'gondoic-acid_100g', 'mead-acid_100g', 'erucic-acid_100g', 'nervonic-acid_100g', 'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g', 'added-sugars_100g', 'sucrose_100g', 'glucose_100g', 'fructose_100g', 'lactose_100g', 'maltose_100g', 'maltodextrins_100g', 'starch_100g', 'polyols_100g', 'erythritol_100g', 'fiber_100g', 'soluble-fiber_100g', 'insoluble-fiber_100g', 'proteins_100g', 'casein_100g', 'serum-proteins_100g', 'nucleotides_100g', 'salt_100g', 'added-salt_100g', 'sodium_100g', 'alcohol_100g', 'vitamin-a_100g', 'beta-carotene_100g', 'vitamin-d_100g', 'vitamin-e_100g', 'vitamin-k_100g', 'vitamin-c_100g', 'vitamin-b1_100g', 'vitamin-b2_100g', 'vitamin-pp_100g', 'vitamin-b6_100g', 'vitamin-b9_100g', 'folates_100g', 'vitamin-b12_100g', 'biotin_100g', 'pantothenic-acid_100g', 'silica_100g', 'bicarbonate_100g', 'potassium_100g', 'chloride_100g', 'calcium_100g', 'phosphorus_100g', 'iron_100g', 'magnesium_100g', 'zinc_100g', 'copper_100g', 'manganese_100g', 'fluoride_100g', 'selenium_100g', 'chromium_100g', 'molybdenum_100g', 'iodine_100g', 'caffeine_100g', 'taurine_100g', 'ph_100g', 'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-dried_100g', 'fruits-vegetables-nuts-estimate_100g', 'fruits-vegetables-nuts-estimate-from-ingredients_100g', 'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g', 'carbon-footprint_100g', 'carbon-footprint-from-meat-or-fish_100g', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g', 'glycemic-index_100g', 'water-hardness_100g', 'choline_100g', 'phylloquinone_100g', 'beta-glucan_100g', 'inositol_100g', 'carnitine_100g', 'sulphate_100g', 'nitrate_100g', 'acidity_100g', 'quantity_numeric']\n",
      "Column 'code': 6327 rows detected as outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'created_t': 3302 rows detected as outliers.\n",
      "Column 'last_modified_t': 3453 rows detected as outliers.\n",
      "Column 'last_updated_t': 0 rows detected as outliers.\n",
      "Column 'serving_quantity': 23175 rows detected as outliers.\n",
      "Column 'additives_n': 20688 rows detected as outliers.\n",
      "Column 'nutriscore_score': 68 rows detected as outliers.\n",
      "Column 'nova_group': 21424 rows detected as outliers.\n",
      "Column 'ecoscore_score': 905 rows detected as outliers.\n",
      "Column 'product_quantity': 20553 rows detected as outliers.\n",
      "Column 'unique_scans_n': 30026 rows detected as outliers.\n",
      "Column 'completeness': 39599 rows detected as outliers.\n",
      "Column 'last_image_t': 9984 rows detected as outliers.\n",
      "Column 'energy-kj_100g': 829 rows detected as outliers.\n",
      "Column 'energy-kcal_100g': 8312 rows detected as outliers.\n",
      "Column 'energy_100g': 8359 rows detected as outliers.\n",
      "Column 'energy-from-fat_100g': 22 rows detected as outliers.\n",
      "Column 'fat_100g': 28966 rows detected as outliers.\n",
      "Column 'saturated-fat_100g': 60539 rows detected as outliers.\n",
      "Column 'butyric-acid_100g': 2 rows detected as outliers.\n",
      "Column 'caproic-acid_100g': 1 rows detected as outliers.\n",
      "Column 'caprylic-acid_100g': 0 rows detected as outliers.\n",
      "Column 'capric-acid_100g': 1 rows detected as outliers.\n",
      "Column 'lauric-acid_100g': 0 rows detected as outliers.\n",
      "Column 'myristic-acid_100g': 2 rows detected as outliers.\n",
      "Column 'palmitic-acid_100g': 5 rows detected as outliers.\n",
      "Column 'stearic-acid_100g': 1 rows detected as outliers.\n",
      "Column 'arachidic-acid_100g': 16 rows detected as outliers.\n",
      "Column 'behenic-acid_100g': 4 rows detected as outliers.\n",
      "Column 'lignoceric-acid_100g': 1 rows detected as outliers.\n",
      "Column 'cerotic-acid_100g': 1 rows detected as outliers.\n",
      "Column 'montanic-acid_100g': 2 rows detected as outliers.\n",
      "Column 'melissic-acid_100g': 1 rows detected as outliers.\n",
      "Column 'unsaturated-fat_100g': 5 rows detected as outliers.\n",
      "Column 'monounsaturated-fat_100g': 1603 rows detected as outliers.\n",
      "Column 'omega-9-fat_100g': 4 rows detected as outliers.\n",
      "Column 'polyunsaturated-fat_100g': 1102 rows detected as outliers.\n",
      "Column 'omega-3-fat_100g': 118 rows detected as outliers.\n",
      "Column 'omega-6-fat_100g': 40 rows detected as outliers.\n",
      "Column 'alpha-linolenic-acid_100g': 58 rows detected as outliers.\n",
      "Column 'eicosapentaenoic-acid_100g': 3 rows detected as outliers.\n",
      "Column 'docosahexaenoic-acid_100g': 10 rows detected as outliers.\n",
      "Column 'linoleic-acid_100g': 33 rows detected as outliers.\n",
      "Column 'arachidonic-acid_100g': 8 rows detected as outliers.\n",
      "Column 'gamma-linolenic-acid_100g': 0 rows detected as outliers.\n",
      "Column 'dihomo-gamma-linolenic-acid_100g': 0 rows detected as outliers.\n",
      "Column 'oleic-acid_100g': 4 rows detected as outliers.\n",
      "Column 'elaidic-acid_100g': 0 rows detected as outliers.\n",
      "Column 'gondoic-acid_100g': 5 rows detected as outliers.\n",
      "Column 'mead-acid_100g': 1 rows detected as outliers.\n",
      "Column 'erucic-acid_100g': 1 rows detected as outliers.\n",
      "Column 'nervonic-acid_100g': 0 rows detected as outliers.\n",
      "Column 'trans-fat_100g': 2671 rows detected as outliers.\n",
      "Column 'cholesterol_100g': 11295 rows detected as outliers.\n",
      "Column 'carbohydrates_100g': 376 rows detected as outliers.\n",
      "Column 'sugars_100g': 80629 rows detected as outliers.\n",
      "Column 'added-sugars_100g': 1261 rows detected as outliers.\n",
      "Column 'sucrose_100g': 5 rows detected as outliers.\n",
      "Column 'glucose_100g': 5 rows detected as outliers.\n",
      "Column 'fructose_100g': 0 rows detected as outliers.\n",
      "Column 'lactose_100g': 80 rows detected as outliers.\n",
      "Column 'maltose_100g': 5 rows detected as outliers.\n",
      "Column 'maltodextrins_100g': 10 rows detected as outliers.\n",
      "Column 'starch_100g': 0 rows detected as outliers.\n",
      "Column 'polyols_100g': 0 rows detected as outliers.\n",
      "Column 'erythritol_100g': 3 rows detected as outliers.\n",
      "Column 'fiber_100g': 22374 rows detected as outliers.\n",
      "Column 'soluble-fiber_100g': 33 rows detected as outliers.\n",
      "Column 'insoluble-fiber_100g': 46 rows detected as outliers.\n",
      "Column 'proteins_100g': 27299 rows detected as outliers.\n",
      "Column 'casein_100g': 1 rows detected as outliers.\n",
      "Column 'serum-proteins_100g': 4 rows detected as outliers.\n",
      "Column 'nucleotides_100g': 3 rows detected as outliers.\n",
      "Column 'salt_100g': 33140 rows detected as outliers.\n",
      "Column 'added-salt_100g': 4 rows detected as outliers.\n",
      "Column 'sodium_100g': 32700 rows detected as outliers.\n",
      "Column 'alcohol_100g': 888 rows detected as outliers.\n",
      "Column 'vitamin-a_100g': 6201 rows detected as outliers.\n",
      "Column 'beta-carotene_100g': 3 rows detected as outliers.\n",
      "Column 'vitamin-d_100g': 1509 rows detected as outliers.\n",
      "Column 'vitamin-e_100g': 197 rows detected as outliers.\n",
      "Column 'vitamin-k_100g': 76 rows detected as outliers.\n",
      "Column 'vitamin-c_100g': 8715 rows detected as outliers.\n",
      "Column 'vitamin-b1_100g': 531 rows detected as outliers.\n",
      "Column 'vitamin-b2_100g': 723 rows detected as outliers.\n",
      "Column 'vitamin-pp_100g': 526 rows detected as outliers.\n",
      "Column 'vitamin-b6_100g': 398 rows detected as outliers.\n",
      "Column 'vitamin-b9_100g': 260 rows detected as outliers.\n",
      "Column 'folates_100g': 104 rows detected as outliers.\n",
      "Column 'vitamin-b12_100g': 485 rows detected as outliers.\n",
      "Column 'biotin_100g': 62 rows detected as outliers.\n",
      "Column 'pantothenic-acid_100g': 197 rows detected as outliers.\n",
      "Column 'silica_100g': 11 rows detected as outliers.\n",
      "Column 'bicarbonate_100g': 53 rows detected as outliers.\n",
      "Column 'potassium_100g': 2951 rows detected as outliers.\n",
      "Column 'chloride_100g': 74 rows detected as outliers.\n",
      "Column 'calcium_100g': 6265 rows detected as outliers.\n",
      "Column 'phosphorus_100g': 189 rows detected as outliers.\n",
      "Column 'iron_100g': 4223 rows detected as outliers.\n",
      "Column 'magnesium_100g': 383 rows detected as outliers.\n",
      "Column 'zinc_100g': 347 rows detected as outliers.\n",
      "Column 'copper_100g': 91 rows detected as outliers.\n",
      "Column 'manganese_100g': 64 rows detected as outliers.\n",
      "Column 'fluoride_100g': 27 rows detected as outliers.\n",
      "Column 'selenium_100g': 90 rows detected as outliers.\n",
      "Column 'chromium_100g': 20 rows detected as outliers.\n",
      "Column 'molybdenum_100g': 18 rows detected as outliers.\n",
      "Column 'iodine_100g': 126 rows detected as outliers.\n",
      "Column 'caffeine_100g': 115 rows detected as outliers.\n",
      "Column 'taurine_100g': 9 rows detected as outliers.\n",
      "Column 'ph_100g': 13 rows detected as outliers.\n",
      "Column 'fruits-vegetables-nuts_100g': 0 rows detected as outliers.\n",
      "Column 'fruits-vegetables-nuts-dried_100g': 62 rows detected as outliers.\n",
      "Column 'fruits-vegetables-nuts-estimate_100g': 1 rows detected as outliers.\n",
      "Column 'fruits-vegetables-nuts-estimate-from-ingredients_100g': 38870 rows detected as outliers.\n",
      "Column 'collagen-meat-protein-ratio_100g': 23 rows detected as outliers.\n",
      "Column 'cocoa_100g': 0 rows detected as outliers.\n",
      "Column 'chlorophyl_100g': 0 rows detected as outliers.\n",
      "Column 'carbon-footprint_100g': 8 rows detected as outliers.\n",
      "Column 'carbon-footprint-from-meat-or-fish_100g': 239 rows detected as outliers.\n",
      "Column 'nutrition-score-fr_100g': 68 rows detected as outliers.\n",
      "Column 'nutrition-score-uk_100g': 0 rows detected as outliers.\n",
      "Column 'glycemic-index_100g': 0 rows detected as outliers.\n",
      "Column 'water-hardness_100g': 0 rows detected as outliers.\n",
      "Column 'choline_100g': 2 rows detected as outliers.\n",
      "Column 'phylloquinone_100g': 55 rows detected as outliers.\n",
      "Column 'beta-glucan_100g': 1 rows detected as outliers.\n",
      "Column 'inositol_100g': 2 rows detected as outliers.\n",
      "Column 'carnitine_100g': 2 rows detected as outliers.\n",
      "Column 'sulphate_100g': 8 rows detected as outliers.\n",
      "Column 'nitrate_100g': 5 rows detected as outliers.\n",
      "Column 'acidity_100g': 0 rows detected as outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'quantity_numeric': 10045 rows detected as outliers.\n",
      "Total rows before filtering: 702611\n",
      "Total rows removed as outliers: 586842\n",
      "Total rows remaining: 115769\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.261814Z",
     "iopub.status.busy": "2024-12-02T15:45:10.261447Z",
     "iopub.status.idle": "2024-12-02T15:45:10.284147Z",
     "shell.execute_reply": "2024-12-02T15:45:10.282118Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.261780Z"
    }
   },
   "outputs": [],
   "source": [
    "# display the schema of the cleaned DataFrame\n",
    "df_outliers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select columns to keep\n",
    "selected_column = [\n",
    "    'code',\n",
    "    'product_name',\n",
    "    'brands',\n",
    "    'categories',\n",
    "    \"main_category\",\n",
    "    'quantity',\n",
    "    'packaging',\n",
    "    'countries',\n",
    "    'ingredients_text',\n",
    "    'allergens',\n",
    "    'serving_size',\n",
    "    'energy-kcal_100g',\n",
    "    'fat_100g',\n",
    "    'saturated-fat_100g',\n",
    "    \"proteins_100g\",\n",
    "    'sugars_100g',\n",
    "    'salt_100g',\n",
    "    'nutriscore_score',\n",
    "    'nutriscore_grade',\n",
    "    \"food_groups_en\",\n",
    "    \"sodium\",\n",
    "    \"sugar\",\n",
    "    \"fiber\"\n",
    "]\n",
    "\n",
    "df_transformed = df_parquet.select(selected_column)\n",
    "df_transformed.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the columns to the appropriate format\n",
    "column_to_convert = [\"quantity\", \"nutriscore_score\", \"energy-kcal_100g\",\n",
    "                     \"fat_100g\", \"saturated-fat_100g\", \"proteins_100g\", \"sugars_100g\", \"salt_100g\"]\n",
    "# apply the conversion\n",
    "for column in column_to_convert:\n",
    "    df_transformed = df_transformed.withColumn(column, col(column).cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert code in string\n",
    "df_transformed = df_transformed.withColumn(\"code\", col(\"code\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation des données Transform :\n",
    "Ajouter des colonnes calculées, par exemple : Indice de qualité nutritionnelle \n",
    "Calculer un score basé sur les nutriments (e.g., sodium, sugar, fiber). \n",
    "Extraire la catégorie principale d'un produit (e.g., \"boissons\", \"snacks\"). \n",
    "Regrouper les données par catégories (categories) pour analyser les tendances (e.g., moyenne des calories par catégorie).\n",
    "\n",
    "--> Quel calcules effectuer ?  \n",
    "--> Quel catégories créer ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.286878Z",
     "iopub.status.busy": "2024-12-02T15:45:10.286383Z",
     "iopub.status.idle": "2024-12-02T15:45:10.293513Z",
     "shell.execute_reply": "2024-12-02T15:45:10.292051Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.286822Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse exploratoire :\n",
    "Utiliser des fonctions de calcul sur fenêtre pour : \n",
    "Trouver les produits les plus caloriques par catégorie. \n",
    "Identifier les tendances de production par brands (marques). \n",
    "Générer des statistiques descriptives (e.g., médiane, moyenne des nutriments par catégorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.295951Z",
     "iopub.status.busy": "2024-12-02T15:45:10.295452Z",
     "iopub.status.idle": "2024-12-02T15:45:10.310323Z",
     "shell.execute_reply": "2024-12-02T15:45:10.309191Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.295896Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde des données Save :\n",
    "Partitionner les données par catégories (categories) et années (year). \n",
    "Sauvegarder les résultats transformés en format Parquet avec compression Snappy. \n",
    "Sauvegarder les résultats transformés dans les bases de données: postgresql/sqlserver/mysql/Snowflake/BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.312547Z",
     "iopub.status.busy": "2024-12-02T15:45:10.312130Z",
     "iopub.status.idle": "2024-12-02T15:45:10.325396Z",
     "shell.execute_reply": "2024-12-02T15:45:10.324250Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.312512Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Sauvegarde des données (load)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Présentation des résultats :\n",
    "Visualiser les résultats sous forme de graphiques ou tableaux \n",
    "(les étudiants peuvent utiliser un outil comme Jupyter Notebook en local ou Google Colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.327347Z",
     "iopub.status.busy": "2024-12-02T15:45:10.326959Z",
     "iopub.status.idle": "2024-12-02T15:45:10.340748Z",
     "shell.execute_reply": "2024-12-02T15:45:10.339541Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.327311Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Présentation des données\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6107906,
     "sourceId": 9935398,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
