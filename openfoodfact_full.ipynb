{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explication des cellules d'initialisation\n",
    "Nous avons testé plusieurs approches : en local, avec Pycharm. Dans le cloud, avec Kaggle & Google Colab.\n",
    "Nous avons donc mis en place plusieurs configurations pour chaque environnement.\n",
    "Si local, vérifier que java 11 est bien installé. Si Kaggle, uploader le csv comme dataset afin de l'avoir à disposition.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T11:48:27.352438Z",
     "start_time": "2024-12-03T11:48:27.347270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import shutil\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install pyspark\n",
    "file_path_csv = \"/kaggle/input/openfoodfacts/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"/kaggle/working/en.openfoodfacts.org.products.parquet\""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T11:34:49.157522Z",
     "start_time": "2024-12-03T11:34:49.155068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path_csv = \"./data/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"./data/en.openfoodfacts.org.products.parquet\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T11:51:37.024576Z",
     "start_time": "2024-12-03T11:50:31.551189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Démarrage du script...\")\n",
    "\n",
    "# Initialiser une SparkSession avec des logs réduits\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exploration OpenFoodFacts\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\")  \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Réduction des logs\n",
    "\n",
    "print(\"PySpark chargé\")\n",
    "\n",
    "try:\n",
    "    # Charger le fichier CSV en tant que DataFrame Spark puis échantillonne 20%\n",
    "    df_csv_before_sample = spark.read.csv(file_path_csv, header=True, inferSchema=True, sep=\"\\t\")\n",
    "    print(\"Fichier CSV chargé.\")\n",
    "    df_csv = df_csv_before_sample.sample(withReplacement=False, fraction=0.2)  # Échantillonnage à 20%\n",
    "    print(\"Echantillonage terminé\")\n",
    "\n",
    "    # Sauvegarder le DataFrame au format Parquet\n",
    "    df_csv.write.parquet(file_path_parquet, mode=\"overwrite\")\n",
    "    print(\"Données sauvegardées au format Parquet.\")\n",
    "\n",
    "    # Charger le fichier Parquet pour une analyse future\n",
    "    df_parquet = spark.read.parquet(file_path_parquet)\n",
    "    print(\"Fichier Parquet chargé.\")\n",
    "\n",
    "    # Vérifier et supprimer l'ancienne table Hive\n",
    "    table_name = \"hive_table\"\n",
    "    hive_table_path = f\"spark-warehouse/{table_name}\"\n",
    "    if table_name in [t.name for t in spark.catalog.listTables(\"default\")]:\n",
    "        print(f\"La table '{table_name}' existe déjà et doit être supprimé.\")\n",
    "        spark.sql(f\"DROP TABLE {table_name}\")\n",
    "        print(f\"Table Hive '{table_name}' supprimée.\")\n",
    "    if os.path.exists(hive_table_path):\n",
    "        shutil.rmtree(hive_table_path)\n",
    "        print(f\"Répertoire associé '{hive_table_path}' supprimé.\")\n",
    "\n",
    "    # Création de la table Hive\n",
    "    print(\"Création et insertion dans la table Hive...\")\n",
    "    hive_table_start_time = time.time()\n",
    "    df_csv.write.mode(\"overwrite\").saveAsTable(\"hive_table\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur rencontrée : {e}\")\n",
    "\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Temps d'exécution : {elapsed_time:.2f} secondes\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage du script...\n",
      "PySpark chargé\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier CSV chargé.\n",
      "Echantillonage terminé\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données sauvegardées au format Parquet.\n",
      "Fichier Parquet chargé.\n",
      "La table 'hive_table' existe déjà et doit être supprimé.\n",
      "Table Hive 'hive_table' supprimée.\n",
      "Création et insertion dans la table Hive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:====================================================>   (71 + 5) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution : 65.47 secondes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T11:37:05.682042Z",
     "start_time": "2024-12-03T11:37:01.426673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Mesure du temps pour compter les lignes du DataFrame CSV\n",
    "csv_start_time = time.time()\n",
    "csv_row_count = df_csv.count()  # Compter les lignes\n",
    "csv_end_time = time.time()\n",
    "csv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\n",
    "print(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes du DataFrame Parquet\n",
    "parquet_start_time = time.time()\n",
    "parquet_row_count = df_parquet.count()  # Compter les lignes\n",
    "parquet_end_time = time.time()\n",
    "parquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\n",
    "print(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes de la table Hive\n",
    "hive_start_time = time.time()\n",
    "df_hive = spark.sql(\"SELECT * FROM hive_table\")  # Charger la table Hive\n",
    "hive_row_count = df_hive.count()  # Compter les lignes\n",
    "hive_end_time = time.time()\n",
    "hive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\n",
    "print(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n",
    "\n",
    "# Comparaison des temps\n",
    "print(\"\\nComparaison des performances :\")\n",
    "print(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV (comptage des lignes): 4102.313 ms - Nombre de lignes: 703178\n",
      "Parquet (comptage des lignes): 117.488 ms - Nombre de lignes: 703178\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_table` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [hive_table], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Mesure du temps pour compter les lignes de la table Hive\u001B[39;00m\n\u001B[1;32m     16\u001B[0m hive_start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m---> 17\u001B[0m df_hive \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSELECT * FROM hive_table\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Charger la table Hive\u001B[39;00m\n\u001B[1;32m     18\u001B[0m hive_row_count \u001B[38;5;241m=\u001B[39m df_hive\u001B[38;5;241m.\u001B[39mcount()  \u001B[38;5;66;03m# Compter les lignes\u001B[39;00m\n\u001B[1;32m     19\u001B[0m hive_end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[0;32m~/anaconda3/envs/ETL_project/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1627\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1628\u001B[0m         litArgs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoArray(\n\u001B[1;32m   1629\u001B[0m             [_to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m [])]\n\u001B[1;32m   1630\u001B[0m         )\n\u001B[0;32m-> 1631\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1632\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1633\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/envs/ETL_project/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/anaconda3/envs/ETL_project/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_table` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [hive_table], [], false\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse préliminaire\n",
    "### 1. Mise en valeur des lignes, colonnes\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mesure du temps pour compter les lignes du DataFrame CSV\n",
    "csv_start_time = time.time()\n",
    "csv_row_count = df_csv.count()\n",
    "csv_end_time = time.time()\n",
    "csv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\n",
    "print(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes du DataFrame Parquet\n",
    "parquet_start_time = time.time()\n",
    "parquet_row_count = df_parquet.count()\n",
    "parquet_end_time = time.time()\n",
    "parquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\n",
    "print(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes de la table Hive\n",
    "hive_start_time = time.time()\n",
    "df_hive = spark.sql(\"SELECT * FROM hive_table\")\n",
    "hive_row_count = df_hive.count()\n",
    "hive_end_time = time.time()\n",
    "hive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\n",
    "print(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n",
    "\n",
    "# Comparaison des temps\n",
    "print(\"\\nComparaison des performances :\")\n",
    "print(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### .2 Gestion des valeurs manquantes\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate missing data percentage for each column\n",
    "total_rows = df_parquet.count()\n",
    "missing_data = (\n",
    "    df_parquet.select([\n",
    "        (count(when(col(c).isNull() | (col(c) == \"\"), c)) / total_rows).alias(c)\n",
    "        for c in df_parquet.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Transform columns into rows (melt operation)\n",
    "missing_data_melted = missing_data.selectExpr(\n",
    "    \"stack({0}, {1}) as (Column, MissingPercentage)\".format(\n",
    "        len(df_parquet.columns),\n",
    "        \", \".join([f\"'{col}', `{col}`\" for col in df_parquet.columns])\n",
    "    )\n",
    ").filter(col(\"MissingPercentage\").isNotNull()).orderBy(col(\"MissingPercentage\").desc())\n",
    "\n",
    "# Identify columns with 100% missing data\n",
    "columns_to_drop = (\n",
    "    missing_data_melted.filter(col(\"MissingPercentage\") == 1.0)\n",
    "    .select(\"Column\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Drop columns with 100% missing values\n",
    "df_cleanedby_missing_value = df_parquet.drop(*columns_to_drop)\n",
    "\n",
    "# Display the top 10 columns with the highest missing percentages\n",
    "print(\"Top 10 columns with the highest missing percentages:\")\n",
    "missing_data_melted.show(10, truncate=False)\n",
    "\n",
    "# Print dropped columns\n",
    "print(f\"Columns dropped due to 100% missing values: {columns_to_drop}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Gestion des valeurs doublons"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Analyzing Duplicates in 'code', 'product_name', and 'brands'\n",
    "duplicates = (\n",
    "    df_cleanedby_missing_value.groupBy(\"code\", \"product_name\", \"brands\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "# Affiche le nombre de doublons identifiés\n",
    "print(f\"There are {duplicates.count()} duplicate rows based on 'code', 'product_name', and 'brands'.\")\n",
    "duplicates.show(truncate=False)\n",
    "\n",
    "# Remove duplicates where 'code', 'product_name', and 'brands' are the same\n",
    "df_cleaned_by_duplicate = df_cleanedby_missing_value.dropDuplicates([\"code\", \"product_name\", \"brands\"])\n",
    "\n",
    "print(f\"Number of rows before removing duplicates: {df_cleanedby_missing_value.count()}\")\n",
    "print(f\"Number of rows after removing duplicates: {df_cleaned_by_duplicate.count()}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Handle outliers"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType\n",
    "\n",
    "# Extraire les valeurs numériques de la colonne \"quantity\"\n",
    "df_cleaned_by_outliers = df_cleaned_by_duplicate.withColumn(\n",
    "    \"quantity_numeric\",\n",
    "    regexp_extract(col(\"quantity\"), r\"(\\d+)\", 1).cast(\"double\")\n",
    ")\n",
    "\n",
    "# Détecter les colonnes numériques\n",
    "numeric_columns = [\n",
    "    field.name for field in df_cleaned_by_outliers.schema.fields\n",
    "    if isinstance(field.dataType, (IntegerType, DoubleType, FloatType))\n",
    "]\n",
    "print(f\"Numeric columns detected: {numeric_columns}\")\n",
    "\n",
    "if not numeric_columns:\n",
    "    print(\"No numeric columns found. Please check your data.\")\n",
    "else:\n",
    "    total_rows_before = df_cleaned_by_outliers.count()\n",
    "    removed_rows_total = 0\n",
    "\n",
    "    # Boucle sur les colonnes numériques pour détecter les outliers\n",
    "    for column in numeric_columns:\n",
    "        try:\n",
    "            # Calcul des quartiles avec approxQuantile\n",
    "            quantiles = df_cleaned_by_outliers.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "            if len(quantiles) < 2:\n",
    "                print(f\"Column '{column}' has insufficient data. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            q1, q3 = quantiles\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "            # Filtrer les outliers\n",
    "            df_outliers = df_cleaned_by_outliers.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "            removed_rows = df_outliers.count()\n",
    "            removed_rows_total += removed_rows\n",
    "\n",
    "            print(f\"Column '{column}': {removed_rows} rows detected as outliers.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column '{column}': {e}\")\n",
    "\n",
    "    print(f\"Total rows before filtering: {total_rows_before}\")\n",
    "    print(f\"Total rows removed as outliers: {removed_rows_total}\")\n",
    "    print(f\"Total rows remaining: {total_rows_before - removed_rows_total}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data cleaning\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T11:24:45.910907Z",
     "start_time": "2024-12-03T11:24:45.017367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# display the schema of the cleaned DataFrame\n",
    "df_parquet = df_outliers\n",
    "df_parquet.describe()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_outliers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# display the schema of the cleaned DataFrame\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_parquet \u001B[38;5;241m=\u001B[39m \u001B[43mdf_outliers\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_outliers' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#select columns to keep\n",
    "selected_column = [\n",
    "    'code',\n",
    "    'product_name',\n",
    "    'brands',\n",
    "    'categories',\n",
    "    \"main_category\",\n",
    "    'quantity',\n",
    "    'packaging',\n",
    "    'countries',\n",
    "    'ingredients_text',\n",
    "    'allergens',\n",
    "    'serving_size',\n",
    "    'energy-kcal_100g',\n",
    "    'fat_100g',\n",
    "    'saturated-fat_100g',\n",
    "    \"proteins_100g\",\n",
    "    'sugars_100g',\n",
    "    'salt_100g',\n",
    "    'nutriscore_score',\n",
    "    'nutriscore_grade',\n",
    "    \"food_groups_en\",\n",
    "    \"sodium\",\n",
    "    \"sugar\",\n",
    "    \"fiber\"\n",
    "]\n",
    "\n",
    "df_transformed = df_parquet.select(selected_column)\n",
    "df_transformed.show(5, truncate=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert the columns to the appropriate format\n",
    "column_to_convert = [\"quantity\", \"nutriscore_score\", \"energy-kcal_100g\",\n",
    "                     \"fat_100g\", \"saturated-fat_100g\", \"proteins_100g\", \"sugars_100g\", \"salt_100g\"]\n",
    "# apply the conversion\n",
    "for column in column_to_convert:\n",
    "    df_transformed = df_transformed.withColumn(column, col(column).cast(\"double\"))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert code in string\n",
    "df_transformed = df_transformed.withColumn(\"code\", col(\"code\").cast(\"string\"))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformation des données Transform :\n",
    "Ajouter des colonnes calculées, par exemple : Indice de qualité nutritionnelle \n",
    "Calculer un score basé sur les nutriments (e.g., sodium, sugar, fiber). \n",
    "Extraire la catégorie principale d'un produit (e.g., \"boissons\", \"snacks\"). \n",
    "Regrouper les données par catégories (categories) pour analyser les tendances (e.g., moyenne des calories par catégorie).\n",
    "\n",
    "--> Quel calcules effectuer ?  \n",
    "--> Quel catégories créer ?\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Transformation\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse exploratoire :\n",
    "Utiliser des fonctions de calcul sur fenêtre pour : \n",
    "Trouver les produits les plus caloriques par catégorie. \n",
    "Identifier les tendances de production par brands (marques). \n",
    "Générer des statistiques descriptives (e.g., médiane, moyenne des nutriments par catégorie"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Exploration\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sauvegarde des données Save :\n",
    "Partitionner les données par catégories (categories) et années (year). \n",
    "Sauvegarder les résultats transformés en format Parquet avec compression Snappy. \n",
    "Sauvegarder les résultats transformés dans les bases de données: postgresql/sqlserver/mysql/Snowflake/BigQuery"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Sauvegarde des données (load)\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Présentation des résultats :\n",
    "Visualiser les résultats sous forme de graphiques ou tableaux \n",
    "(les étudiants peuvent utiliser un outil comme Jupyter Notebook en local ou Google Colab "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Présentation des données\")"
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6107906,
     "sourceId": 9935398,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
