{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 9935398,
     "sourceType": "datasetVersion",
     "datasetId": 6107906
    }
   ],
   "dockerImageVersionId": 30786,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Extraction des données ( via le package installé via \"pip install pyspark\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# **Prérequis :\n### Installer pip install pyspark via la gestion des dépendances de kaggle\n### Attendre la fin du chargement du csv\n### ???\n### Profit !\n\n### Objectif :\n# Trier les données afin de récupérer celles qui nous interessent.",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path_csv = \"/kaggle/input/openfoodfacts/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"/kaggle/working/en.openfoodfacts.org.products.parquet\""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T09:55:27.706394Z",
     "start_time": "2024-12-03T09:55:27.674881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path_csv = \"./data/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"./data/en.openfoodfacts.org.products.parquet\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "start_time = time.time()\n",
    "print(\"Démarrage du script...\")\n",
    "\n",
    "# Initialiser une SparkSession avec des logs réduits\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exploration OpenFoodFacts\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Réduction des logs\n",
    "\n",
    "print(\"PySpark chargé\")\n",
    "\n",
    "try:\n",
    "    # Charger le fichier CSV en tant que DataFrame Spark puis échantillonne 20%\n",
    "    df_csv_before_sample = spark.read.csv(file_path_csv, header=True, inferSchema=True, sep=\"\\t\")\n",
    "    print(\"Fichier CSV chargé.\")\n",
    "    df_csv = df_csv_before_sample.sample(withReplacement=False, fraction=0.2)  # Échantillonnage à 20%\n",
    "    print(\"Echantillonage terminé\")\n",
    "\n",
    "    # Sauvegarder le DataFrame au format Parquet\n",
    "    df_csv.write.parquet(file_path_parquet, mode=\"overwrite\")\n",
    "    print(\"Données sauvegardées au format Parquet.\")\n",
    "\n",
    "    # Charger le fichier Parquet pour une analyse future\n",
    "    df_parquet = spark.read.parquet(file_path_parquet)\n",
    "    print(\"Fichier Parquet chargé.\")\n",
    "\n",
    "    # Création de la table Hive\n",
    "    print(\"Création et insertion dans la table Hive...\")\n",
    "    hive_table_start_time = time.time()\n",
    "\n",
    "    # Écrire les données dans la table Hive\n",
    "    df_csv.write.mode(\"overwrite\").saveAsTable(\"hive_table\")\n",
    "    \n",
    "except:\n",
    "    print(\"ERRRRROOOOOOR\")\n",
    "\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Temps d'exécution : {elapsed_time:.2f} secondes\")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-02T15:56:13.599891Z",
     "iopub.execute_input": "2024-12-02T15:56:13.600421Z",
     "iopub.status.idle": "2024-12-02T16:01:02.589980Z",
     "shell.execute_reply.started": "2024-12-02T15:56:13.600381Z",
     "shell.execute_reply": "2024-12-02T16:01:02.588311Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.5.3)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nDémarrage du script...\nPySpark chargé\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Fichier CSV chargé.\nEchantillonage terminé\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Données sauvegardées au format Parquet.\nFichier Parquet chargé.\nCréation et insertion dans la table Hive...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "[Stage 169:======================================================>(75 + 1) / 76]\r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Temps d'exécution : 277.31 secondes\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                \r",
     "output_type": "stream"
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "source": "# Mesure du temps pour compter les lignes du DataFrame CSV\ncsv_start_time = time.time()\ncsv_row_count = df_csv.count()  # Compter les lignes\ncsv_end_time = time.time()\ncsv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\nprint(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n\n# Mesure du temps pour compter les lignes du DataFrame Parquet\nparquet_start_time = time.time()\nparquet_row_count = df_parquet.count()  # Compter les lignes\nparquet_end_time = time.time()\nparquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\nprint(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n\n# Mesure du temps pour compter les lignes de la table Hive\nhive_start_time = time.time()\ndf_hive = spark.sql(\"SELECT * FROM hive_table\")  # Charger la table Hive\nhive_row_count = df_hive.count()  # Compter les lignes\nhive_end_time = time.time()\nhive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\nprint(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n\n# Comparaison des temps\nprint(\"\\nComparaison des performances :\")\nprint(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\nprint(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\nprint(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T16:05:59.018167Z",
     "iopub.execute_input": "2024-12-02T16:05:59.018727Z",
     "iopub.status.idle": "2024-12-02T16:06:20.319739Z",
     "shell.execute_reply.started": "2024-12-02T16:05:59.018674Z",
     "shell.execute_reply": "2024-12-02T16:06:20.318332Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "                                                                                \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "CSV (comptage des lignes): 20648.751 ms - Nombre de lignes: 702094\nParquet (comptage des lignes): 243.271 ms - Nombre de lignes: 702094\nHive (comptage des lignes): 390.368 ms - Nombre de lignes: 702094\n\nComparaison des performances :\nCSV Execution Time: 20648.751 ms\nParquet Execution Time: 243.271 ms\nHive Execution Time: 390.368 ms\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "source": "# Preliminary Analysis\n### 1. Highlight the number of columns, rows, and the list of column names\n\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Mesurer le temps pour le DataFrame Parquet\nparquet_start_time = time.time()\nparquet_schema = df_parquet.schema  # Obtenir le schéma\nparquet_columns = len(df_parquet.columns)  # Nombre de colonnes\nparquet_rows = df_parquet.count()  # Nombre de lignes\nparquet_elapsed_time_ms = (time.time() - parquet_start_time) * 1000\n\n# Mesurer le temps pour le DataFrame CSV\ncsv_start_time = time.time()\ncsv_schema = df_csv.schema  # Obtenir le schéma\ncsv_columns = len(df_csv.columns)  # Nombre de colonnes\ncsv_rows = df_csv.count()  # Nombre de lignes\ncsv_elapsed_time_ms = (time.time() - csv_start_time) * 1000\n\n# Afficher les résultats\nprint(\"\\nRésultats pour le fichier Parquet:\")\n\nprint(f\"Nombre de colonnes (Parquet): {parquet_columns}\")\nprint(f\"Nombre de lignes (Parquet): {parquet_rows}\")\nprint(f\"Temps total d'exécution (Parquet): {parquet_elapsed_time_ms:.2f} ms\")\n\nprint(\"\\nRésultats pour le fichier CSV:\")\n\nprint(f\"Nombre de colonnes (CSV): {csv_columns}\")\nprint(f\"Nombre de lignes (CSV): {csv_rows}\")\nprint(f\"Temps total d'exécution (CSV): {csv_elapsed_time_ms:.2f} ms\")\n\n# Comparaison des temps\nprint(\"\\nComparaison des performances:\")\nprint(f\"Temps d'exécution CSV: {csv_elapsed_time_ms:.2f} ms\")\nprint(f\"Temps d'exécution Parquet: {parquet_elapsed_time_ms:.2f} ms\")\nif csv_elapsed_time_ms < parquet_elapsed_time_ms:\n    print(\"CSV est plus rapide.\")\nelse:\n    print(\"Parquet est plus rapide.\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T16:11:02.603921Z",
     "iopub.execute_input": "2024-12-02T16:11:02.604345Z",
     "iopub.status.idle": "2024-12-02T16:11:17.785795Z",
     "shell.execute_reply.started": "2024-12-02T16:11:02.604308Z",
     "shell.execute_reply": "2024-12-02T16:11:17.784529Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "[Stage 188:=====================================================> (74 + 2) / 76]\r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\nRésultats pour le fichier Parquet:\nNombre de colonnes (Parquet): 206\nNombre de lignes (Parquet): 702094\nTemps total d'exécution (Parquet): 239.64 ms\n\nRésultats pour le fichier CSV:\nNombre de colonnes (CSV): 206\nNombre de lignes (CSV): 702094\nTemps total d'exécution (CSV): 14932.49 ms\n\nComparaison des performances:\nTemps d'exécution CSV: 14932.49 ms\nTemps d'exécution Parquet: 239.64 ms\nParquet est plus rapide.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                \r",
     "output_type": "stream"
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "source": "### .2 Handling Missing Values\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pyspark.sql.functions import col, count, when\n\nstart_time = time.time()\n\n# Calculate missing data percentage for each column\ntotal_rows = df_parquet.count() \nmissing_data = (\n    df_parquet.select([\n        (count(when(col(c).isNull() | (col(c) == \"\"), c)) / total_rows).alias(c)\n        for c in df_parquet.columns\n    ])\n)\n\n# Transform columns into rows (melt operation)\nmissing_data_melted = missing_data.selectExpr(\n    \"stack({0}, {1}) as (Column, MissingPercentage)\".format(\n        len(df_parquet.columns),\n        \", \".join([f\"'{col}', `{col}`\" for col in df_parquet.columns])\n    )\n).filter(col(\"MissingPercentage\").isNotNull()).orderBy(col(\"MissingPercentage\").desc())\n\n# Identify columns with 100% missing data\ncolumns_to_drop = (\n    missing_data_melted.filter(col(\"MissingPercentage\") == 1.0)\n    .select(\"Column\")\n    .rdd.flatMap(lambda x: x)\n    .collect()\n)\n\n# Drop columns with 100% missing values\ndf_cleaned = df_parquet.drop(*columns_to_drop)\n\n# Display the top 10 columns with the highest missing percentages\nprint(\"Top 10 columns with the highest missing percentages:\")\nmissing_data_melted.show(10, truncate=False)\n\n# Print dropped columns\nprint(f\"Columns dropped due to 100% missing values: {columns_to_drop}\")\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Execution completed in {elapsed_time:.2f} seconds\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T15:43:13.149548Z",
     "iopub.execute_input": "2024-12-02T15:43:13.150037Z",
     "iopub.status.idle": "2024-12-02T15:44:54.449482Z",
     "shell.execute_reply.started": "2024-12-02T15:43:13.149982Z",
     "shell.execute_reply": "2024-12-02T15:44:54.448127Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "                                                                                \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Top 10 columns with the highest missing percentages:\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "[Stage 147:==================================================>    (10 + 1) / 11]\r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "+-----------------------+------------------+\n|Column                 |MissingPercentage |\n+-----------------------+------------------+\n|cities                 |1.0               |\n|allergens_en           |1.0               |\n|additives              |0.9999991461323352|\n|nutrition-score-uk_100g|0.9999991461323352|\n|elaidic-acid_100g      |0.9999980076421155|\n|glycemic-index_100g    |0.9999977230195607|\n|chlorophyl_100g        |0.9999974383970057|\n|erucic-acid_100g       |0.9999968691518959|\n|water-hardness_100g    |0.999996584529341 |\n|caproic-acid_100g      |0.9999954460391214|\n+-----------------------+------------------+\nonly showing top 10 rows\n\nColumns dropped due to 100% missing values: ['cities', 'allergens_en']\nExecution completed in 101.28 seconds\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                \r",
     "output_type": "stream"
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Handling Duplicates",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pyspark.sql.functions import col, count\n\nstart_time = time.time()\n\n# Analyzing Duplicates in 'code', 'product_name', and 'brands'\nduplicates = (\n    df_parquet.groupBy(\"code\", \"product_name\", \"brands\")\n    .count()\n    .filter(col(\"count\") > 1)\n)\n\n# Affiche le nombre de doublons identifiés\nprint(f\"There are {duplicates.count()} duplicate rows based on 'code', 'product_name', and 'brands'.\")\nduplicates.show(truncate=False)\n\n# Remove duplicates where 'code', 'product_name', and 'brands' are the same\ndf_cleaned = df_parquet.dropDuplicates([\"code\", \"product_name\", \"brands\"])\n\nprint(f\"Number of rows after removing duplicates: {df_cleaned.count()}\")\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Execution completed in {elapsed_time:.2f} seconds\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T15:44:54.460345Z",
     "iopub.execute_input": "2024-12-02T15:44:54.460788Z",
     "iopub.status.idle": "2024-12-02T15:45:10.159777Z",
     "shell.execute_reply.started": "2024-12-02T15:44:54.460741Z",
     "shell.execute_reply": "2024-12-02T15:45:10.157123Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "                                                                                \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "There are 1968 duplicate rows based on 'code', 'product_name', and 'brands'.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "+-----------------+------------------------------------------------------------+-----------------------------+-----+\n|code             |product_name                                                |brands                       |count|\n+-----------------+------------------------------------------------------------+-----------------------------+-----+\n|3.245413820389E12|Sauce hollandaise                                           |Carrefour                    |2    |\n|3.24541503968E12 |Ile de Beauté Rosé                                          |Réserve de Padulone,Carrefour|2    |\n|3.245415141093E12|320G Beignet Chocolat X 4 Blis                              |Carrefour                    |2    |\n|3.245677723693E12|Espresso Auchan Bio                                         |Auchan                       |2    |\n|3.250390058991E12|Printiligne                                                 |Paturages                    |2    |\n|3.250390109808E12|Chips nature                                                |Bouton d'or                  |2    |\n|3.250390167112E12|Blanc de poulet doré au four                                |Monique Ranou                |2    |\n|3.250391110322E12|Soupe Savoyarde                                             |Netto                        |2    |\n|3.250391311897E12|Côte de porc                                                |Jean roze                    |2    |\n|3.250391505036E12|Mon gourmand supérieur sans couenne                         |Monique Ranou                |2    |\n|3.250391600465E12|Source Faustine                                             |Netto                        |2    |\n|3.250391751266E12|Vinaigre de cidre bio                                       |Bouton d'Or                  |2    |\n|3.250391759873E12|Le charolais Burger viande Charolaise sauce au poivre, 195 g|Monique ranou                |2    |\n|3.25039199623E12 |Armagnac V.S.                                               |Hubert Rochebois             |2    |\n|3.250392056797E12|Pave au poivre 6 tranches                                   |FE P&C                       |2    |\n|3.250392171018E12|Poulet curry                                                |Monique Ranou                |2    |\n|3.250392273453E12|Champignons de paris émincés bio                            |Saint Eloi                   |2    |\n|3.25039233695E12 |Saucisson porc a l ail                                      |FE P&C                       |2    |\n|3.250392483906E12|Compote                                                     |Paquito                      |2    |\n|3.250392487348E12|Boisson thé infusé citron                                   |Look, Cotterley              |2    |\n+-----------------+------------------------------------------------------------+-----------------------------+-----+\nonly showing top 20 rows\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "[Stage 161:===================>                                     (2 + 4) / 6]\r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Number of rows after removing duplicates: 3511425\nExecution completed in 15.68 seconds\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                \r",
     "output_type": "stream"
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Handle outliers",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pyspark.sql.functions import regexp_extract\n\ndf_parquet = df_parquet.withColumn(\n    \"quantity_numeric\",\n    regexp_extract(col(\"quantity\"), r\"(\\d+)\", 1).cast(\"double\")\n)\n\nnumeric_columns = [\n    field.name for field in df_parquet.schema.fields \n    if str(field.dataType) in [\"IntegerType\", \"DoubleType\", \"FloatType\"]\n]\nprint(f\"Numeric columns detected: {numeric_columns}\")\n\nif not numeric_columns:\n    print(\"No numeric columns found. Please check your data.\")\nelse:\n    # Boucle sur les colonnes numériques pour détecter les outliers\n    for column in numeric_columns:\n        try:\n            quantiles = df_parquet.approxQuantile(column, [0.25, 0.75], 0.1)\n            if len(quantiles) < 2:\n                print(f\"Column '{column}' has insufficient data. Skipping...\")\n                continue\n            \n            q1, q3 = quantiles\n            iqr = q3 - q1\n            lower_bound = q1 - 1.5 * iqr\n            upper_bound = q3 + 1.5 * iqr\n\n            print(f\"Column: {column}\")\n            print(f\"Q1: {q1}, Q3: {q3}, IQR: {iqr}\")\n            print(f\"Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n            \n            outliers = df_parquet.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n            print(f\"Outliers detected in '{column}': {outliers.count()}\")\n\n        except Exception as e:\n            print(f\"Error processing column '{column}': {e}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T15:45:10.171558Z",
     "iopub.execute_input": "2024-12-02T15:45:10.172010Z",
     "iopub.status.idle": "2024-12-02T15:45:10.259967Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.171958Z",
     "shell.execute_reply": "2024-12-02T15:45:10.258603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Numeric columns detected: []\nNo numeric columns found. Please check your data.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": "# Data cleaning\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# display the schema of the cleaned DataFrame\n",
    "df_parquet.describe()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T15:45:10.261447Z",
     "iopub.execute_input": "2024-12-02T15:45:10.261814Z",
     "iopub.status.idle": "2024-12-02T15:45:10.284147Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.261780Z",
     "shell.execute_reply": "2024-12-02T15:45:10.282118Z"
    }
   },
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#select columns to keep\n",
    "selected_column = [\n",
    "    'code',\n",
    "    'product_name',\n",
    "    'brands',\n",
    "    'categories',\n",
    "    \"main_category\",\n",
    "    'quantity',\n",
    "    'packaging',\n",
    "    'countries',\n",
    "    'ingredients_text',\n",
    "    'allergens',\n",
    "    'serving_size',\n",
    "    'energy-kcal_100g',\n",
    "    'fat_100g',\n",
    "    'saturated-fat_100g',\n",
    "    \"proteins_100g\",\n",
    "    'sugars_100g',\n",
    "    'salt_100g',\n",
    "    'nutriscore_score',\n",
    "    'nutriscore_grade',\n",
    "    \"food_groups_en\",\n",
    "]\n",
    "\n",
    "df_transformed = df_parquet.select(selected_column)\n",
    "df_transformed.show(5, truncate=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert the columns to the appropriate format\n",
    "column_to_convert = [\"quantity\", \"nutriscore_score\", \"energy-kcal_100g\",\n",
    "                     \"fat_100g\", \"saturated-fat_100g\", \"proteins_100g\", \"sugars_100g\", \"salt_100g\"]\n",
    "# apply the conversion\n",
    "for column in column_to_convert:\n",
    "    df_transformed = df_transformed.withColumn(column, col(column).cast(\"double\"))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert code in string\n",
    "df_transformed = df_transformed.withColumn(\"code\", col(\"code\").cast(\"string\"))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Transformation des données Transform :\nAjouter des colonnes calculées, par exemple : Indice de qualité nutritionnelle \nCalculer un score basé sur les nutriments (e.g., sodium, sugar, fiber). \nExtraire la catégorie principale d'un produit (e.g., \"boissons\", \"snacks\"). \nRegrouper les données par catégories (categories) pour analyser les tendances (e.g., moyenne des calories par catégorie).\n\n--> Quel calcules effectuer ?  \n--> Quel catégories créer ?\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Transformation\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T15:45:10.286383Z",
     "iopub.execute_input": "2024-12-02T15:45:10.286878Z",
     "iopub.status.idle": "2024-12-02T15:45:10.293513Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.286822Z",
     "shell.execute_reply": "2024-12-02T15:45:10.292051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Transformation\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "source": "# Analyse exploratoire :\nUtiliser des fonctions de calcul sur fenêtre pour : \nTrouver les produits les plus caloriques par catégorie. \nIdentifier les tendances de production par brands (marques). \nGénérer des statistiques descriptives (e.g., médiane, moyenne des nutriments par catégorie",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Exploration\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T15:45:10.295452Z",
     "iopub.execute_input": "2024-12-02T15:45:10.295951Z",
     "iopub.status.idle": "2024-12-02T15:45:10.310323Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.295896Z",
     "shell.execute_reply": "2024-12-02T15:45:10.309191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Exploration\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": "# Sauvegarde des données Save :\nPartitionner les données par catégories (categories) et années (year). \nSauvegarder les résultats transformés en format Parquet avec compression Snappy. \nSauvegarder les résultats transformés dans les bases de données: postgresql/sqlserver/mysql/Snowflake/BigQuery",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Sauvegarde des données (load)\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T15:45:10.312130Z",
     "iopub.execute_input": "2024-12-02T15:45:10.312547Z",
     "iopub.status.idle": "2024-12-02T15:45:10.325396Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.312512Z",
     "shell.execute_reply": "2024-12-02T15:45:10.324250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Sauvegarde des données (load)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": "\n\n# Présentation des résultats :\nVisualiser les résultats sous forme de graphiques ou tableaux \n(les étudiants peuvent utiliser un outil comme Jupyter Notebook en local ou Google Colab ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Présentation des données\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T15:45:10.326959Z",
     "iopub.execute_input": "2024-12-02T15:45:10.327347Z",
     "iopub.status.idle": "2024-12-02T15:45:10.340748Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.327311Z",
     "shell.execute_reply": "2024-12-02T15:45:10.339541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Présentation des données\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 65
  }
 ]
}
