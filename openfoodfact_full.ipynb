{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explication des cellules d'initialisation\n",
    "Nous avons test√© plusieurs approches : en local, avec Pycharm. Dans le cloud, avec Kaggle & Google Colab.\n",
    "Nous avons donc mis en place plusieurs configurations pour chaque environnement.\n",
    "Si local, v√©rifier que java 11 est bien install√©. Si Kaggle, uploader le csv comme dataset afin de l'avoir √† disposition.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T10:29:08.996141Z",
     "start_time": "2024-12-03T10:29:08.822047Z"
    }
   },
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "file_path_csv = \"/kaggle/input/openfoodfacts/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"/kaggle/working/en.openfoodfacts.org.products.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T10:29:17.789399Z",
     "start_time": "2024-12-03T10:29:17.786574Z"
    }
   },
   "source": [
    "file_path_csv = \"./data/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"./data/en.openfoodfacts.org.products.parquet\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T10:30:40.540323Z",
     "start_time": "2024-12-03T10:29:22.335944Z"
    }
   },
   "source": [
    "start_time = time.time()\n",
    "print(\"D√©marrage du script...\")\n",
    "\n",
    "# Initialiser une SparkSession avec des logs r√©duits\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exploration OpenFoodFacts\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # R√©duction des logs\n",
    "\n",
    "print(\"PySpark charg√©\")\n",
    "\n",
    "try:\n",
    "    # Charger le fichier CSV en tant que DataFrame Spark puis √©chantillonne 20%\n",
    "    df_csv_before_sample = spark.read.csv(file_path_csv, header=True, inferSchema=True, sep=\"\\t\")\n",
    "    print(\"Fichier CSV charg√©.\")\n",
    "    df_csv = df_csv_before_sample.sample(withReplacement=False, fraction=0.2)  # √âchantillonnage √† 20%\n",
    "    print(\"Echantillonage termin√©\")\n",
    "\n",
    "    # Sauvegarder le DataFrame au format Parquet\n",
    "    df_csv.write.parquet(file_path_parquet, mode=\"overwrite\")\n",
    "    print(\"Donn√©es sauvegard√©es au format Parquet.\")\n",
    "\n",
    "    # Charger le fichier Parquet pour une analyse future\n",
    "    df_parquet = spark.read.parquet(file_path_parquet)\n",
    "    print(\"Fichier Parquet charg√©.\")\n",
    "\n",
    "    # Cr√©ation de la table Hive\n",
    "    print(\"Cr√©ation et insertion dans la table Hive...\")\n",
    "    hive_table_start_time = time.time()\n",
    "\n",
    "    # √âcrire les donn√©es dans la table Hive\n",
    "    df_csv.write.mode(\"overwrite\").saveAsTable(\"hive_table\")\n",
    "    \n",
    "except:\n",
    "    print(\"ERRRRROOOOOOR\")\n",
    "\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Temps d'ex√©cution : {elapsed_time:.2f} secondes\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©marrage du script...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 11:29:23 WARN Utils: Your hostname, cedric-galaxy resolves to a loopback address: 127.0.1.1; using 192.168.1.26 instead (on interface wlo1)\n",
      "24/12/03 11:29:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/03 11:29:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark charg√©\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier CSV charg√©.\n",
      "Echantillonage termin√©\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donn√©es sauvegard√©es au format Parquet.\n",
      "Fichier Parquet charg√©.\n",
      "Cr√©ation et insertion dans la table Hive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================================>      (67 + 9) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'ex√©cution : 78.20 secondes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T16:05:59.018727Z",
     "iopub.status.busy": "2024-12-02T16:05:59.018167Z",
     "iopub.status.idle": "2024-12-02T16:06:20.319739Z",
     "shell.execute_reply": "2024-12-02T16:06:20.318332Z",
     "shell.execute_reply.started": "2024-12-02T16:05:59.018674Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-03T10:35:35.393502Z",
     "start_time": "2024-12-03T10:35:28.125350Z"
    }
   },
   "source": [
    "# Mesure du temps pour compter les lignes du DataFrame CSV\n",
    "csv_start_time = time.time()\n",
    "csv_row_count = df_csv.count()  # Compter les lignes\n",
    "csv_end_time = time.time()\n",
    "csv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\n",
    "print(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes du DataFrame Parquet\n",
    "parquet_start_time = time.time()\n",
    "parquet_row_count = df_parquet.count()  # Compter les lignes\n",
    "parquet_end_time = time.time()\n",
    "parquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\n",
    "print(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes de la table Hive\n",
    "hive_start_time = time.time()\n",
    "df_hive = spark.sql(\"SELECT * FROM hive_table\")  # Charger la table Hive\n",
    "hive_row_count = df_hive.count()  # Compter les lignes\n",
    "hive_end_time = time.time()\n",
    "hive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\n",
    "print(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n",
    "\n",
    "# Comparaison des temps\n",
    "print(\"\\nComparaison des performances :\")\n",
    "print(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV (comptage des lignes): 6113.584 ms - Nombre de lignes: 702698\n",
      "Parquet (comptage des lignes): 569.154 ms - Nombre de lignes: 702698\n",
      "Hive (comptage des lignes): 577.465 ms - Nombre de lignes: 702698\n",
      "\n",
      "Comparaison des performances :\n",
      "CSV Execution Time: 6113.584 ms\n",
      "Parquet Execution Time: 569.154 ms\n",
      "Hive Execution Time: 577.465 ms\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse pr√©liminaire\n",
    "### 1. Mise en valeur des lignes, colonnes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T16:11:02.604345Z",
     "iopub.status.busy": "2024-12-02T16:11:02.603921Z",
     "iopub.status.idle": "2024-12-02T16:11:17.785795Z",
     "shell.execute_reply": "2024-12-02T16:11:17.784529Z",
     "shell.execute_reply.started": "2024-12-02T16:11:02.604308Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-03T10:38:16.971741Z",
     "start_time": "2024-12-03T10:38:12.325074Z"
    }
   },
   "source": [
    "# Mesure du temps pour compter les lignes du DataFrame CSV\n",
    "csv_start_time = time.time()\n",
    "csv_row_count = df_csv.count()\n",
    "csv_end_time = time.time()\n",
    "csv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\n",
    "print(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes du DataFrame Parquet\n",
    "parquet_start_time = time.time()\n",
    "parquet_row_count = df_parquet.count()\n",
    "parquet_end_time = time.time()\n",
    "parquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\n",
    "print(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes de la table Hive\n",
    "hive_start_time = time.time()\n",
    "df_hive = spark.sql(\"SELECT * FROM hive_table\")\n",
    "hive_row_count = df_hive.count()\n",
    "hive_end_time = time.time()\n",
    "hive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\n",
    "print(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n",
    "\n",
    "# Comparaison des temps\n",
    "print(\"\\nComparaison des performances :\")\n",
    "print(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV (comptage des lignes): 4385.785 ms - Nombre de lignes: 702698\n",
      "Parquet (comptage des lignes): 117.743 ms - Nombre de lignes: 702698\n",
      "Hive (comptage des lignes): 137.143 ms - Nombre de lignes: 702698\n",
      "\n",
      "Comparaison des performances :\n",
      "CSV Execution Time: 4385.785 ms\n",
      "Parquet Execution Time: 117.743 ms\n",
      "Hive Execution Time: 137.143 ms\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### .2 Gestion des valeurs manquantes\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:43:13.150037Z",
     "iopub.status.busy": "2024-12-02T15:43:13.149548Z",
     "iopub.status.idle": "2024-12-02T15:44:54.449482Z",
     "shell.execute_reply": "2024-12-02T15:44:54.448127Z",
     "shell.execute_reply.started": "2024-12-02T15:43:13.149982Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-03T11:08:20.975810Z",
     "start_time": "2024-12-03T11:08:05.014829Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate missing data percentage for each column\n",
    "total_rows = df_parquet.count()\n",
    "missing_data = (\n",
    "    df_parquet.select([\n",
    "        (count(when(col(c).isNull() | (col(c) == \"\"), c)) / total_rows).alias(c)\n",
    "        for c in df_parquet.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Transform columns into rows (melt operation)\n",
    "missing_data_melted = missing_data.selectExpr(\n",
    "    \"stack({0}, {1}) as (Column, MissingPercentage)\".format(\n",
    "        len(df_parquet.columns),\n",
    "        \", \".join([f\"'{col}', `{col}`\" for col in df_parquet.columns])\n",
    "    )\n",
    ").filter(col(\"MissingPercentage\").isNotNull()).orderBy(col(\"MissingPercentage\").desc())\n",
    "\n",
    "# Identify columns with 100% missing data\n",
    "columns_to_drop = (\n",
    "    missing_data_melted.filter(col(\"MissingPercentage\") == 1.0)\n",
    "    .select(\"Column\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Drop columns with 100% missing values\n",
    "df_cleanedby_missing_value = df_parquet.drop(*columns_to_drop)\n",
    "\n",
    "# Display the top 10 columns with the highest missing percentages\n",
    "print(\"Top 10 columns with the highest missing percentages:\")\n",
    "missing_data_melted.show(10, truncate=False)\n",
    "\n",
    "# Print dropped columns\n",
    "print(f\"Columns dropped due to 100% missing values: {columns_to_drop}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 columns with the highest missing percentages:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 729:====================>                                  (6 + 10) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------+\n",
      "|Column                          |MissingPercentage |\n",
      "+--------------------------------+------------------+\n",
      "|cities                          |1.0               |\n",
      "|allergens_en                    |1.0               |\n",
      "|additives                       |0.9999985769135532|\n",
      "|nutrition-score-uk_100g         |0.9999985769135532|\n",
      "|nervonic-acid_100g              |0.9999971538271064|\n",
      "|elaidic-acid_100g               |0.9999971538271064|\n",
      "|chlorophyl_100g                 |0.9999971538271064|\n",
      "|water-hardness_100g             |0.9999971538271064|\n",
      "|gamma-linolenic-acid_100g       |0.9999957307406596|\n",
      "|dihomo-gamma-linolenic-acid_100g|0.9999957307406596|\n",
      "+--------------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Columns dropped due to 100% missing values: ['cities', 'allergens_en']\n",
      "Execution completed in 15.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3. Gestion des valeurs doublons"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:44:54.460788Z",
     "iopub.status.busy": "2024-12-02T15:44:54.460345Z",
     "iopub.status.idle": "2024-12-02T15:45:10.159777Z",
     "shell.execute_reply": "2024-12-02T15:45:10.157123Z",
     "shell.execute_reply.started": "2024-12-02T15:44:54.460741Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-03T11:12:57.842361Z",
     "start_time": "2024-12-03T11:12:55.507229Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Analyzing Duplicates in 'code', 'product_name', and 'brands'\n",
    "duplicates = (\n",
    "    df_cleanedby_missing_value.groupBy(\"code\", \"product_name\", \"brands\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "# Affiche le nombre de doublons identifi√©s\n",
    "print(f\"There are {duplicates.count()} duplicate rows based on 'code', 'product_name', and 'brands'.\")\n",
    "duplicates.show(truncate=False)\n",
    "\n",
    "# Remove duplicates where 'code', 'product_name', and 'brands' are the same\n",
    "df_cleaned_by_duplicate = df_cleanedby_missing_value.dropDuplicates([\"code\", \"product_name\", \"brands\"])\n",
    "\n",
    "print(f\"Number of rows before removing duplicates: {df_cleanedby_missing_value.count()}\")\n",
    "print(f\"Number of rows after removing duplicates: {df_cleaned_by_duplicate.count()}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 87 duplicate rows based on 'code', 'product_name', and 'brands'.\n",
      "+---------------------+-----------------------------------------------------+----------------------------------------+-----+\n",
      "|code                 |product_name                                         |brands                                  |count|\n",
      "+---------------------+-----------------------------------------------------+----------------------------------------+-----+\n",
      "|3.250393046759E12    |Porc marin√© longue conservation                      |Jean roze                               |2    |\n",
      "|3.25456643534E12     |ERREUR_IMAGES                                        |Pierre Chanau,Auchan                    |2    |\n",
      "|3.25456649037E12     |Saucisses de volaille                                |Auchan                                  |2    |\n",
      "|3.596710344819E12    |R√¥ti de Porc cuit sup√©rieur                          |Auchan                                  |2    |\n",
      "|3.596710408924E12    |Chiffonnade de jambon cuit                           |Auchan                                  |2    |\n",
      "|3.661273099503E12    |Bananes 5 doigts Cavendish Cat. 1                    |Premier Prix                            |2    |\n",
      "|8.594071424626E12    |Tartare saumon                                       |Carrefour                               |2    |\n",
      "|2.3327665001305117E22|Riz long parfum√©                                     |Riz du monde                            |2    |\n",
      "|3.250391535224E12    |Paupiettes de porc                                   |Jean roze                               |2    |\n",
      "|3.560070778669E12    |Cocktail fruit and nuts                              |Carrefour                               |2    |\n",
      "|3.560071424343E12    |Oh juice (Pur Jus Multifruitsüççüçåüçä) & Eau de Cocoü••)|Carrefour                               |2    |\n",
      "|2.03050868536E12     |Baguette a la farine aveyron                         |Auchan                                  |2    |\n",
      "|3.596710170111E12    |Rik & Rok - Mini go√ªters saveur chocolat             |Rik & Rok,Auchan                        |2    |\n",
      "|5.8612990221900134E17|Fin√≠simo de pechuga de pavo                          |Bonarea                                 |2    |\n",
      "|1.2211917E7          |alphabet                                             |netto                                   |2    |\n",
      "|3.417960015451E12    |Herbes de Provence                                   |Cook,Arcadie                            |2    |\n",
      "|3.523680307797E12    |4 Micro Donuts Choco                                 |Carrefour                               |2    |\n",
      "|3.017800199428E12    |Flageolets bio, 265g                                 |D'Aucy                                  |2    |\n",
      "|8.012666504121E12    |Offelle di parona                                    |Terre d'Italia, Terre ditalia, Carrefour|2    |\n",
      "|8.012666011476E12    |NULL                                                 |Carrefour                               |2    |\n",
      "+---------------------+-----------------------------------------------------+----------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows before removing duplicates: 702698\n",
      "Number of rows after removing duplicates: 702611\n",
      "Execution completed in 2.33 seconds\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.172010Z",
     "iopub.status.busy": "2024-12-02T15:45:10.171558Z",
     "iopub.status.idle": "2024-12-02T15:45:10.259967Z",
     "shell.execute_reply": "2024-12-02T15:45:10.258603Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.171958Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-03T11:11:00.783427Z",
     "start_time": "2024-12-03T11:08:44.131976Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType\n",
    "\n",
    "# Extraire les valeurs num√©riques de la colonne \"quantity\"\n",
    "df_cleaned_by_outliers = df_cleaned_by_duplicate.withColumn(\n",
    "    \"quantity_numeric\",\n",
    "    regexp_extract(col(\"quantity\"), r\"(\\d+)\", 1).cast(\"double\")\n",
    ")\n",
    "\n",
    "# D√©tecter les colonnes num√©riques\n",
    "numeric_columns = [\n",
    "    field.name for field in df_cleaned_by_outliers.schema.fields\n",
    "    if isinstance(field.dataType, (IntegerType, DoubleType, FloatType))\n",
    "]\n",
    "print(f\"Numeric columns detected: {numeric_columns}\")\n",
    "\n",
    "if not numeric_columns:\n",
    "    print(\"No numeric columns found. Please check your data.\")\n",
    "else:\n",
    "    total_rows_before = df_cleaned_by_outliers.count()\n",
    "    removed_rows_total = 0\n",
    "\n",
    "    # Boucle sur les colonnes num√©riques pour d√©tecter les outliers\n",
    "    for column in numeric_columns:\n",
    "        try:\n",
    "            # Calcul des quartiles avec approxQuantile\n",
    "            quantiles = df_cleaned_by_outliers.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "            if len(quantiles) < 2:\n",
    "                print(f\"Column '{column}' has insufficient data. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            q1, q3 = quantiles\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "            # Filtrer les outliers\n",
    "            df_outliers = df_cleaned_by_outliers.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "            removed_rows = df_outliers.count()\n",
    "            removed_rows_total += removed_rows\n",
    "\n",
    "            print(f\"Column '{column}': {removed_rows} rows detected as outliers.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column '{column}': {e}\")\n",
    "\n",
    "    print(f\"Total rows before filtering: {total_rows_before}\")\n",
    "    print(f\"Total rows removed as outliers: {removed_rows_total}\")\n",
    "    print(f\"Total rows remaining: {total_rows_before - removed_rows_total}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns detected: ['code', 'created_t', 'last_modified_t', 'last_updated_t', 'serving_quantity', 'additives_n', 'nutriscore_score', 'nova_group', 'ecoscore_score', 'product_quantity', 'unique_scans_n', 'completeness', 'last_image_t', 'energy-kj_100g', 'energy-kcal_100g', 'energy_100g', 'energy-from-fat_100g', 'fat_100g', 'saturated-fat_100g', 'butyric-acid_100g', 'caproic-acid_100g', 'caprylic-acid_100g', 'capric-acid_100g', 'lauric-acid_100g', 'myristic-acid_100g', 'palmitic-acid_100g', 'stearic-acid_100g', 'arachidic-acid_100g', 'behenic-acid_100g', 'lignoceric-acid_100g', 'cerotic-acid_100g', 'montanic-acid_100g', 'melissic-acid_100g', 'unsaturated-fat_100g', 'monounsaturated-fat_100g', 'omega-9-fat_100g', 'polyunsaturated-fat_100g', 'omega-3-fat_100g', 'omega-6-fat_100g', 'alpha-linolenic-acid_100g', 'eicosapentaenoic-acid_100g', 'docosahexaenoic-acid_100g', 'linoleic-acid_100g', 'arachidonic-acid_100g', 'gamma-linolenic-acid_100g', 'dihomo-gamma-linolenic-acid_100g', 'oleic-acid_100g', 'elaidic-acid_100g', 'gondoic-acid_100g', 'mead-acid_100g', 'erucic-acid_100g', 'nervonic-acid_100g', 'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g', 'added-sugars_100g', 'sucrose_100g', 'glucose_100g', 'fructose_100g', 'lactose_100g', 'maltose_100g', 'maltodextrins_100g', 'starch_100g', 'polyols_100g', 'erythritol_100g', 'fiber_100g', 'soluble-fiber_100g', 'insoluble-fiber_100g', 'proteins_100g', 'casein_100g', 'serum-proteins_100g', 'nucleotides_100g', 'salt_100g', 'added-salt_100g', 'sodium_100g', 'alcohol_100g', 'vitamin-a_100g', 'beta-carotene_100g', 'vitamin-d_100g', 'vitamin-e_100g', 'vitamin-k_100g', 'vitamin-c_100g', 'vitamin-b1_100g', 'vitamin-b2_100g', 'vitamin-pp_100g', 'vitamin-b6_100g', 'vitamin-b9_100g', 'folates_100g', 'vitamin-b12_100g', 'biotin_100g', 'pantothenic-acid_100g', 'silica_100g', 'bicarbonate_100g', 'potassium_100g', 'chloride_100g', 'calcium_100g', 'phosphorus_100g', 'iron_100g', 'magnesium_100g', 'zinc_100g', 'copper_100g', 'manganese_100g', 'fluoride_100g', 'selenium_100g', 'chromium_100g', 'molybdenum_100g', 'iodine_100g', 'caffeine_100g', 'taurine_100g', 'ph_100g', 'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-dried_100g', 'fruits-vegetables-nuts-estimate_100g', 'fruits-vegetables-nuts-estimate-from-ingredients_100g', 'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g', 'carbon-footprint_100g', 'carbon-footprint-from-meat-or-fish_100g', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g', 'glycemic-index_100g', 'water-hardness_100g', 'choline_100g', 'phylloquinone_100g', 'beta-glucan_100g', 'inositol_100g', 'carnitine_100g', 'sulphate_100g', 'nitrate_100g', 'acidity_100g', 'quantity_numeric']\n",
      "Column 'code': 6327 rows detected as outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'created_t': 3302 rows detected as outliers.\n",
      "Column 'last_modified_t': 3453 rows detected as outliers.\n",
      "Column 'last_updated_t': 0 rows detected as outliers.\n",
      "Column 'serving_quantity': 23175 rows detected as outliers.\n",
      "Column 'additives_n': 20688 rows detected as outliers.\n",
      "Column 'nutriscore_score': 68 rows detected as outliers.\n",
      "Column 'nova_group': 21424 rows detected as outliers.\n",
      "Column 'ecoscore_score': 905 rows detected as outliers.\n",
      "Column 'product_quantity': 20553 rows detected as outliers.\n",
      "Column 'unique_scans_n': 30026 rows detected as outliers.\n",
      "Column 'completeness': 39599 rows detected as outliers.\n",
      "Column 'last_image_t': 9984 rows detected as outliers.\n",
      "Column 'energy-kj_100g': 829 rows detected as outliers.\n",
      "Column 'energy-kcal_100g': 8312 rows detected as outliers.\n",
      "Column 'energy_100g': 8359 rows detected as outliers.\n",
      "Column 'energy-from-fat_100g': 22 rows detected as outliers.\n",
      "Column 'fat_100g': 28966 rows detected as outliers.\n",
      "Column 'saturated-fat_100g': 60539 rows detected as outliers.\n",
      "Column 'butyric-acid_100g': 2 rows detected as outliers.\n",
      "Column 'caproic-acid_100g': 1 rows detected as outliers.\n",
      "Column 'caprylic-acid_100g': 0 rows detected as outliers.\n",
      "Column 'capric-acid_100g': 1 rows detected as outliers.\n",
      "Column 'lauric-acid_100g': 0 rows detected as outliers.\n",
      "Column 'myristic-acid_100g': 2 rows detected as outliers.\n",
      "Column 'palmitic-acid_100g': 5 rows detected as outliers.\n",
      "Column 'stearic-acid_100g': 1 rows detected as outliers.\n",
      "Column 'arachidic-acid_100g': 16 rows detected as outliers.\n",
      "Column 'behenic-acid_100g': 4 rows detected as outliers.\n",
      "Column 'lignoceric-acid_100g': 1 rows detected as outliers.\n",
      "Column 'cerotic-acid_100g': 1 rows detected as outliers.\n",
      "Column 'montanic-acid_100g': 2 rows detected as outliers.\n",
      "Column 'melissic-acid_100g': 1 rows detected as outliers.\n",
      "Column 'unsaturated-fat_100g': 5 rows detected as outliers.\n",
      "Column 'monounsaturated-fat_100g': 1603 rows detected as outliers.\n",
      "Column 'omega-9-fat_100g': 4 rows detected as outliers.\n",
      "Column 'polyunsaturated-fat_100g': 1102 rows detected as outliers.\n",
      "Column 'omega-3-fat_100g': 118 rows detected as outliers.\n",
      "Column 'omega-6-fat_100g': 40 rows detected as outliers.\n",
      "Column 'alpha-linolenic-acid_100g': 58 rows detected as outliers.\n",
      "Column 'eicosapentaenoic-acid_100g': 3 rows detected as outliers.\n",
      "Column 'docosahexaenoic-acid_100g': 10 rows detected as outliers.\n",
      "Column 'linoleic-acid_100g': 33 rows detected as outliers.\n",
      "Column 'arachidonic-acid_100g': 8 rows detected as outliers.\n",
      "Column 'gamma-linolenic-acid_100g': 0 rows detected as outliers.\n",
      "Column 'dihomo-gamma-linolenic-acid_100g': 0 rows detected as outliers.\n",
      "Column 'oleic-acid_100g': 4 rows detected as outliers.\n",
      "Column 'elaidic-acid_100g': 0 rows detected as outliers.\n",
      "Column 'gondoic-acid_100g': 5 rows detected as outliers.\n",
      "Column 'mead-acid_100g': 1 rows detected as outliers.\n",
      "Column 'erucic-acid_100g': 1 rows detected as outliers.\n",
      "Column 'nervonic-acid_100g': 0 rows detected as outliers.\n",
      "Column 'trans-fat_100g': 2671 rows detected as outliers.\n",
      "Column 'cholesterol_100g': 11295 rows detected as outliers.\n",
      "Column 'carbohydrates_100g': 376 rows detected as outliers.\n",
      "Column 'sugars_100g': 80629 rows detected as outliers.\n",
      "Column 'added-sugars_100g': 1261 rows detected as outliers.\n",
      "Column 'sucrose_100g': 5 rows detected as outliers.\n",
      "Column 'glucose_100g': 5 rows detected as outliers.\n",
      "Column 'fructose_100g': 0 rows detected as outliers.\n",
      "Column 'lactose_100g': 80 rows detected as outliers.\n",
      "Column 'maltose_100g': 5 rows detected as outliers.\n",
      "Column 'maltodextrins_100g': 10 rows detected as outliers.\n",
      "Column 'starch_100g': 0 rows detected as outliers.\n",
      "Column 'polyols_100g': 0 rows detected as outliers.\n",
      "Column 'erythritol_100g': 3 rows detected as outliers.\n",
      "Column 'fiber_100g': 22374 rows detected as outliers.\n",
      "Column 'soluble-fiber_100g': 33 rows detected as outliers.\n",
      "Column 'insoluble-fiber_100g': 46 rows detected as outliers.\n",
      "Column 'proteins_100g': 27299 rows detected as outliers.\n",
      "Column 'casein_100g': 1 rows detected as outliers.\n",
      "Column 'serum-proteins_100g': 4 rows detected as outliers.\n",
      "Column 'nucleotides_100g': 3 rows detected as outliers.\n",
      "Column 'salt_100g': 33140 rows detected as outliers.\n",
      "Column 'added-salt_100g': 4 rows detected as outliers.\n",
      "Column 'sodium_100g': 32700 rows detected as outliers.\n",
      "Column 'alcohol_100g': 888 rows detected as outliers.\n",
      "Column 'vitamin-a_100g': 6201 rows detected as outliers.\n",
      "Column 'beta-carotene_100g': 3 rows detected as outliers.\n",
      "Column 'vitamin-d_100g': 1509 rows detected as outliers.\n",
      "Column 'vitamin-e_100g': 197 rows detected as outliers.\n",
      "Column 'vitamin-k_100g': 76 rows detected as outliers.\n",
      "Column 'vitamin-c_100g': 8715 rows detected as outliers.\n",
      "Column 'vitamin-b1_100g': 531 rows detected as outliers.\n",
      "Column 'vitamin-b2_100g': 723 rows detected as outliers.\n",
      "Column 'vitamin-pp_100g': 526 rows detected as outliers.\n",
      "Column 'vitamin-b6_100g': 398 rows detected as outliers.\n",
      "Column 'vitamin-b9_100g': 260 rows detected as outliers.\n",
      "Column 'folates_100g': 104 rows detected as outliers.\n",
      "Column 'vitamin-b12_100g': 485 rows detected as outliers.\n",
      "Column 'biotin_100g': 62 rows detected as outliers.\n",
      "Column 'pantothenic-acid_100g': 197 rows detected as outliers.\n",
      "Column 'silica_100g': 11 rows detected as outliers.\n",
      "Column 'bicarbonate_100g': 53 rows detected as outliers.\n",
      "Column 'potassium_100g': 2951 rows detected as outliers.\n",
      "Column 'chloride_100g': 74 rows detected as outliers.\n",
      "Column 'calcium_100g': 6265 rows detected as outliers.\n",
      "Column 'phosphorus_100g': 189 rows detected as outliers.\n",
      "Column 'iron_100g': 4223 rows detected as outliers.\n",
      "Column 'magnesium_100g': 383 rows detected as outliers.\n",
      "Column 'zinc_100g': 347 rows detected as outliers.\n",
      "Column 'copper_100g': 91 rows detected as outliers.\n",
      "Column 'manganese_100g': 64 rows detected as outliers.\n",
      "Column 'fluoride_100g': 27 rows detected as outliers.\n",
      "Column 'selenium_100g': 90 rows detected as outliers.\n",
      "Column 'chromium_100g': 20 rows detected as outliers.\n",
      "Column 'molybdenum_100g': 18 rows detected as outliers.\n",
      "Column 'iodine_100g': 126 rows detected as outliers.\n",
      "Column 'caffeine_100g': 115 rows detected as outliers.\n",
      "Column 'taurine_100g': 9 rows detected as outliers.\n",
      "Column 'ph_100g': 13 rows detected as outliers.\n",
      "Column 'fruits-vegetables-nuts_100g': 0 rows detected as outliers.\n",
      "Column 'fruits-vegetables-nuts-dried_100g': 62 rows detected as outliers.\n",
      "Column 'fruits-vegetables-nuts-estimate_100g': 1 rows detected as outliers.\n",
      "Column 'fruits-vegetables-nuts-estimate-from-ingredients_100g': 38870 rows detected as outliers.\n",
      "Column 'collagen-meat-protein-ratio_100g': 23 rows detected as outliers.\n",
      "Column 'cocoa_100g': 0 rows detected as outliers.\n",
      "Column 'chlorophyl_100g': 0 rows detected as outliers.\n",
      "Column 'carbon-footprint_100g': 8 rows detected as outliers.\n",
      "Column 'carbon-footprint-from-meat-or-fish_100g': 239 rows detected as outliers.\n",
      "Column 'nutrition-score-fr_100g': 68 rows detected as outliers.\n",
      "Column 'nutrition-score-uk_100g': 0 rows detected as outliers.\n",
      "Column 'glycemic-index_100g': 0 rows detected as outliers.\n",
      "Column 'water-hardness_100g': 0 rows detected as outliers.\n",
      "Column 'choline_100g': 2 rows detected as outliers.\n",
      "Column 'phylloquinone_100g': 55 rows detected as outliers.\n",
      "Column 'beta-glucan_100g': 1 rows detected as outliers.\n",
      "Column 'inositol_100g': 2 rows detected as outliers.\n",
      "Column 'carnitine_100g': 2 rows detected as outliers.\n",
      "Column 'sulphate_100g': 8 rows detected as outliers.\n",
      "Column 'nitrate_100g': 5 rows detected as outliers.\n",
      "Column 'acidity_100g': 0 rows detected as outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'quantity_numeric': 10045 rows detected as outliers.\n",
      "Total rows before filtering: 702611\n",
      "Total rows removed as outliers: 586842\n",
      "Total rows remaining: 115769\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.261814Z",
     "iopub.status.busy": "2024-12-02T15:45:10.261447Z",
     "iopub.status.idle": "2024-12-02T15:45:10.284147Z",
     "shell.execute_reply": "2024-12-02T15:45:10.282118Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.261780Z"
    }
   },
   "outputs": [],
   "source": [
    "# display the schema of the cleaned DataFrame\n",
    "df_outliers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select columns to keep\n",
    "selected_column = [\n",
    "    'code',\n",
    "    'product_name',\n",
    "    'brands',\n",
    "    'categories',\n",
    "    \"main_category\",\n",
    "    'quantity',\n",
    "    'packaging',\n",
    "    'countries',\n",
    "    'ingredients_text',\n",
    "    'allergens',\n",
    "    'serving_size',\n",
    "    'energy-kcal_100g',\n",
    "    'fat_100g',\n",
    "    'saturated-fat_100g',\n",
    "    \"proteins_100g\",\n",
    "    'sugars_100g',\n",
    "    'salt_100g',\n",
    "    'nutriscore_score',\n",
    "    'nutriscore_grade',\n",
    "    \"food_groups_en\",\n",
    "    \"sodium\",\n",
    "    \"sugar\",\n",
    "    \"fiber\"\n",
    "]\n",
    "\n",
    "df_transformed = df_parquet.select(selected_column)\n",
    "df_transformed.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the columns to the appropriate format\n",
    "column_to_convert = [\"quantity\", \"nutriscore_score\", \"energy-kcal_100g\",\n",
    "                     \"fat_100g\", \"saturated-fat_100g\", \"proteins_100g\", \"sugars_100g\", \"salt_100g\"]\n",
    "# apply the conversion\n",
    "for column in column_to_convert:\n",
    "    df_transformed = df_transformed.withColumn(column, col(column).cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert code in string\n",
    "df_transformed = df_transformed.withColumn(\"code\", col(\"code\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation des donn√©es Transform :\n",
    "Ajouter des colonnes calcul√©es, par exemple : Indice de qualit√© nutritionnelle \n",
    "Calculer un score bas√© sur les nutriments (e.g., sodium, sugar, fiber). \n",
    "Extraire la cat√©gorie principale d'un produit (e.g., \"boissons\", \"snacks\"). \n",
    "Regrouper les donn√©es par cat√©gories (categories) pour analyser les tendances (e.g., moyenne des calories par cat√©gorie).\n",
    "\n",
    "--> Quel calcules effectuer ?  \n",
    "--> Quel cat√©gories cr√©er ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.286878Z",
     "iopub.status.busy": "2024-12-02T15:45:10.286383Z",
     "iopub.status.idle": "2024-12-02T15:45:10.293513Z",
     "shell.execute_reply": "2024-12-02T15:45:10.292051Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.286822Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse exploratoire :\n",
    "Utiliser des fonctions de calcul sur fen√™tre pour : \n",
    "Trouver les produits les plus caloriques par cat√©gorie. \n",
    "Identifier les tendances de production par brands (marques). \n",
    "G√©n√©rer des statistiques descriptives (e.g., m√©diane, moyenne des nutriments par cat√©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.295951Z",
     "iopub.status.busy": "2024-12-02T15:45:10.295452Z",
     "iopub.status.idle": "2024-12-02T15:45:10.310323Z",
     "shell.execute_reply": "2024-12-02T15:45:10.309191Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.295896Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde des donn√©es Save :\n",
    "Partitionner les donn√©es par cat√©gories (categories) et ann√©es (year). \n",
    "Sauvegarder les r√©sultats transform√©s en format Parquet avec compression Snappy. \n",
    "Sauvegarder les r√©sultats transform√©s dans les bases de donn√©es: postgresql/sqlserver/mysql/Snowflake/BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.312547Z",
     "iopub.status.busy": "2024-12-02T15:45:10.312130Z",
     "iopub.status.idle": "2024-12-02T15:45:10.325396Z",
     "shell.execute_reply": "2024-12-02T15:45:10.324250Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.312512Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Sauvegarde des donn√©es (load)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Pr√©sentation des r√©sultats :\n",
    "Visualiser les r√©sultats sous forme de graphiques ou tableaux \n",
    "(les √©tudiants peuvent utiliser un outil comme Jupyter Notebook en local ou Google Colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T15:45:10.327347Z",
     "iopub.status.busy": "2024-12-02T15:45:10.326959Z",
     "iopub.status.idle": "2024-12-02T15:45:10.340748Z",
     "shell.execute_reply": "2024-12-02T15:45:10.339541Z",
     "shell.execute_reply.started": "2024-12-02T15:45:10.327311Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Pr√©sentation des donn√©es\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6107906,
     "sourceId": 9935398,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
