{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explication des cellules d'initialisation\n",
    "Nous avons testé plusieurs approches : en local, avec Pycharm. Dans le cloud, avec Kaggle & Google Colab.\n",
    "Nous avons donc mis en place plusieurs configurations pour chaque environnement.\n",
    "Si local, vérifier que java 11 est bien installé. Si Kaggle, uploader le csv comme dataset afin de l'avoir à disposition.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install pyspark\n",
    "file_path_csv = \"/kaggle/input/openfoodfacts/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"/kaggle/working/en.openfoodfacts.org.products.parquet\""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:38:41.469469Z",
     "start_time": "2024-12-03T13:38:41.466152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path_csv = \"./data/en.openfoodfacts.org.products.csv\"\n",
    "file_path_parquet = \"./data/en.openfoodfacts.org.products.parquet\"\n",
    "file_path_mysql = \"/home/cedric/PycharmProjects/ETL_project/mysql-connector-j-9.1.0/mysql-connector-j-9.1.0.jar\"\n"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:40:01.385799Z",
     "start_time": "2024-12-03T13:38:42.628045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Démarrage du script...\")\n",
    "\n",
    "# Initialiser une SparkSession avec des logs réduits\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exploration OpenFoodFacts\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\")  \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.jars\", file_path_mysql) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Réduction des logs\n",
    "\n",
    "print(\"PySpark chargé\")\n",
    "\n",
    "try:\n",
    "    # Charger le fichier CSV en tant que DataFrame Spark puis échantillonne 20%\n",
    "    df_csv_before_sample = spark.read.csv(file_path_csv, header=True, inferSchema=True, sep=\"\\t\")\n",
    "    print(\"Fichier CSV chargé.\")\n",
    "    df_csv = df_csv_before_sample.sample(withReplacement=False, fraction=0.2)  # Échantillonnage à 20%\n",
    "    print(\"Echantillonage terminé\")\n",
    "\n",
    "    # Sauvegarder le DataFrame au format Parquet\n",
    "    df_csv.write.parquet(file_path_parquet, mode=\"overwrite\")\n",
    "    print(\"Données sauvegardées au format Parquet.\")\n",
    "\n",
    "    # Charger le fichier Parquet pour une analyse future\n",
    "    df_parquet = spark.read.parquet(file_path_parquet)\n",
    "    print(\"Fichier Parquet chargé.\")\n",
    "\n",
    "    # Vérifier et supprimer l'ancienne table Hive\n",
    "    table_name = \"hive_table\"\n",
    "    hive_table_path = f\"spark-warehouse/{table_name}\"\n",
    "    if table_name in [t.name for t in spark.catalog.listTables(\"default\")]:\n",
    "        print(f\"La table '{table_name}' existe déjà et doit être supprimé.\")\n",
    "        spark.sql(f\"DROP TABLE {table_name}\")\n",
    "        print(f\"Table Hive '{table_name}' supprimée.\")\n",
    "    if os.path.exists(hive_table_path):\n",
    "        shutil.rmtree(hive_table_path)\n",
    "        print(f\"Répertoire associé '{hive_table_path}' supprimé.\")\n",
    "\n",
    "    # Création de la table Hive\n",
    "    print(\"Création et insertion dans la table Hive...\")\n",
    "    hive_table_start_time = time.time()\n",
    "    df_csv.write.mode(\"overwrite\").saveAsTable(\"hive_table\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur rencontrée : {e}\")\n",
    "\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Temps d'exécution : {elapsed_time:.2f} secondes\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage du script...\n",
      "PySpark chargé\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier CSV chargé.\n",
      "Echantillonage terminé\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données sauvegardées au format Parquet.\n",
      "Fichier Parquet chargé.\n",
      "La table 'hive_table' existe déjà et doit être supprimé.\n",
      "Table Hive 'hive_table' supprimée.\n",
      "Création et insertion dans la table Hive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=======================================================>(75 + 1) / 76]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution : 78.75 secondes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mesure du temps pour compter les lignes du DataFrame CSV\n",
    "csv_start_time = time.time()\n",
    "csv_row_count = df_csv.count()  # Compter les lignes\n",
    "csv_end_time = time.time()\n",
    "csv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\n",
    "print(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes du DataFrame Parquet\n",
    "parquet_start_time = time.time()\n",
    "parquet_row_count = df_parquet.count()  # Compter les lignes\n",
    "parquet_end_time = time.time()\n",
    "parquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\n",
    "print(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes de la table Hive\n",
    "hive_start_time = time.time()\n",
    "df_hive = spark.sql(\"SELECT * FROM hive_table\")  # Charger la table Hive\n",
    "hive_row_count = df_hive.count()  # Compter les lignes\n",
    "hive_end_time = time.time()\n",
    "hive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\n",
    "print(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n",
    "\n",
    "# Comparaison des temps\n",
    "print(\"\\nComparaison des performances :\")\n",
    "print(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse préliminaire\n",
    "### 1. Mise en valeur des lignes, colonnes\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mesure du temps pour compter les lignes du DataFrame CSV\n",
    "csv_start_time = time.time()\n",
    "csv_row_count = df_csv.count()\n",
    "csv_end_time = time.time()\n",
    "csv_elapsed_time_ms = (csv_end_time - csv_start_time) * 1000\n",
    "print(f\"CSV (comptage des lignes): {csv_elapsed_time_ms:.3f} ms - Nombre de lignes: {csv_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes du DataFrame Parquet\n",
    "parquet_start_time = time.time()\n",
    "parquet_row_count = df_parquet.count()\n",
    "parquet_end_time = time.time()\n",
    "parquet_elapsed_time_ms = (parquet_end_time - parquet_start_time) * 1000\n",
    "print(f\"Parquet (comptage des lignes): {parquet_elapsed_time_ms:.3f} ms - Nombre de lignes: {parquet_row_count}\")\n",
    "\n",
    "# Mesure du temps pour compter les lignes de la table Hive\n",
    "hive_start_time = time.time()\n",
    "df_hive = spark.sql(\"SELECT * FROM hive_table\")\n",
    "hive_row_count = df_hive.count()\n",
    "hive_end_time = time.time()\n",
    "hive_elapsed_time_ms = (hive_end_time - hive_start_time) * 1000\n",
    "print(f\"Hive (comptage des lignes): {hive_elapsed_time_ms:.3f} ms - Nombre de lignes: {hive_row_count}\")\n",
    "\n",
    "# Comparaison des temps\n",
    "print(\"\\nComparaison des performances :\")\n",
    "print(f\"CSV Execution Time: {csv_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Parquet Execution Time: {parquet_elapsed_time_ms:.3f} ms\")\n",
    "print(f\"Hive Execution Time: {hive_elapsed_time_ms:.3f} ms\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### .2 Gestion des valeurs manquantes\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate missing data percentage for each column\n",
    "total_rows = df_parquet.count()\n",
    "missing_data = (\n",
    "    df_parquet.select([\n",
    "        (count(when(col(c).isNull() | (col(c) == \"\"), c)) / total_rows).alias(c)\n",
    "        for c in df_parquet.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Transform columns into rows (melt operation)\n",
    "missing_data_melted = missing_data.selectExpr(\n",
    "    \"stack({0}, {1}) as (Column, MissingPercentage)\".format(\n",
    "        len(df_parquet.columns),\n",
    "        \", \".join([f\"'{col}', `{col}`\" for col in df_parquet.columns])\n",
    "    )\n",
    ").filter(col(\"MissingPercentage\").isNotNull()).orderBy(col(\"MissingPercentage\").desc())\n",
    "\n",
    "# Identify columns with 100% missing data\n",
    "columns_to_drop = (\n",
    "    missing_data_melted.filter(col(\"MissingPercentage\") == 1.0)\n",
    "    .select(\"Column\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Drop columns with 100% missing values\n",
    "df_cleanedby_missing_value = df_parquet.drop(*columns_to_drop)\n",
    "\n",
    "# Display the top 10 columns with the highest missing percentages\n",
    "print(\"Top 10 columns with the highest missing percentages:\")\n",
    "missing_data_melted.show(10, truncate=False)\n",
    "\n",
    "# Print dropped columns\n",
    "print(f\"Columns dropped due to 100% missing values: {columns_to_drop}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Gestion des valeurs doublons"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Analyzing Duplicates in 'code', 'product_name', and 'brands'\n",
    "duplicates = (\n",
    "    df_cleanedby_missing_value.groupBy(\"code\", \"product_name\", \"brands\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "# Affiche le nombre de doublons identifiés\n",
    "print(f\"There are {duplicates.count()} duplicate rows based on 'code', 'product_name', and 'brands'.\")\n",
    "duplicates.show(truncate=False)\n",
    "\n",
    "# Remove duplicates where 'code', 'product_name', and 'brands' are the same\n",
    "df_cleaned_by_duplicate = df_cleanedby_missing_value.dropDuplicates([\"code\", \"product_name\", \"brands\"])\n",
    "\n",
    "print(f\"Number of rows before removing duplicates: {df_cleanedby_missing_value.count()}\")\n",
    "print(f\"Number of rows after removing duplicates: {df_cleaned_by_duplicate.count()}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution completed in {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Handle outliers"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType\n",
    "\n",
    "# Extraire les valeurs numériques de la colonne \"quantity\"\n",
    "df_cleaned_by_outliers = df_cleaned_by_duplicate.withColumn(\n",
    "    \"quantity_numeric\",\n",
    "    regexp_extract(col(\"quantity\"), r\"(\\d+)\", 1).cast(\"double\")\n",
    ")\n",
    "\n",
    "# Détecter les colonnes numériques\n",
    "numeric_columns = [\n",
    "    field.name for field in df_cleaned_by_outliers.schema.fields\n",
    "    if isinstance(field.dataType, (IntegerType, DoubleType, FloatType))\n",
    "]\n",
    "print(f\"Numeric columns detected: {numeric_columns}\")\n",
    "\n",
    "if not numeric_columns:\n",
    "    print(\"No numeric columns found. Please check your data.\")\n",
    "else:\n",
    "    total_rows_before = df_cleaned_by_outliers.count()\n",
    "    removed_rows_total = 0\n",
    "\n",
    "    # Boucle sur les colonnes numériques pour détecter les outliers\n",
    "    for column in numeric_columns:\n",
    "        try:\n",
    "            # Calcul des quartiles avec approxQuantile\n",
    "            quantiles = df_cleaned_by_outliers.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "            if len(quantiles) < 2:\n",
    "                print(f\"Column '{column}' has insufficient data. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            q1, q3 = quantiles\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "            # Filtrer les outliers\n",
    "            df_outliers = df_cleaned_by_outliers.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "            removed_rows = df_outliers.count()\n",
    "            removed_rows_total += removed_rows\n",
    "\n",
    "            print(f\"Column '{column}': {removed_rows} rows detected as outliers.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column '{column}': {e}\")\n",
    "\n",
    "    print(f\"Total rows before filtering: {total_rows_before}\")\n",
    "    print(f\"Total rows removed as outliers: {removed_rows_total}\")\n",
    "    print(f\"Total rows remaining: {total_rows_before - removed_rows_total}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data cleaning\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the cleaned DataFrame\n",
    "df_parquet = df_outliers\n",
    "df_parquet.describe()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#select columns to keep\n",
    "selected_column = [\n",
    "    'code',\n",
    "    'product_name',\n",
    "    'brands',\n",
    "    'categories',\n",
    "    \"main_category\",\n",
    "    'quantity',\n",
    "    'packaging',\n",
    "    'countries',\n",
    "    'ingredients_text',\n",
    "    'allergens',\n",
    "    'serving_size',\n",
    "    'energy-kcal_100g',\n",
    "    'fat_100g',\n",
    "    'saturated-fat_100g',\n",
    "    \"proteins_100g\",\n",
    "    'salt_100g',\n",
    "    'nutriscore_score',\n",
    "    'nutriscore_grade',\n",
    "    \"food_groups_en\",\n",
    "    \"sodium_100g\",\n",
    "    \"sugars_100g\",\n",
    "    \"fiber_100g\"\n",
    "]\n",
    "\n",
    "df_transformed = df_parquet.select(selected_column)\n",
    "df_transformed.show(5, truncate=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert the columns to the appropriate format\n",
    "column_to_convert = [\"quantity\", \"nutriscore_score\", \"energy-kcal_100g\",\n",
    "                     \"fat_100g\", \"saturated-fat_100g\", \"proteins_100g\", \"sugars_100g\", \"salt_100g\"]\n",
    "# apply the conversion\n",
    "for column in column_to_convert:\n",
    "    df_transformed = df_transformed.withColumn(column, col(column).cast(\"double\"))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformation des données Transform :\n",
    "Ajouter des colonnes calculées, par exemple : Indice de qualité nutritionnelle \n",
    "Calculer un score basé sur les nutriments (e.g., sodium, sugar, fiber). \n",
    "Extraire la catégorie principale d'un produit (e.g., \"boissons\", \"snacks\"). \n",
    "Regrouper les données par catégories (categories) pour analyser les tendances (e.g., moyenne des calories par catégorie).\n",
    "\n",
    "--> Quel calcules effectuer ?  \n",
    "--> Quel catégories créer ?\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert code in string\n",
    "df_transformed = df_transformed.withColumn(\"code\", col(\"code\").cast(\"string\"))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display the schema of the transformed DataFrame\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# add a new column 'main_category' by extracting the first category from 'categories'\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df_transformed = df_transformed.withColumn(\"main_category\", split(col(\"categories\"), \",\").getItem(0))\n",
    "df_transformed.show(5, truncate=False)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# create a new column with quality nutrition score (IQN)\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"nutrition_score\",\n",
    "    0.3 * col(\"fiber_100g\") +\n",
    "    0.2 * col(\"proteins_100g\") -\n",
    "    0.4 * col(\"sugars_100g\") -\n",
    "    0.3 * col(\"saturated-fat_100g\") -\n",
    "    0.1 * col(\"salt_100g\")\n",
    ")\n",
    "df_transformed.show(5, truncate=False)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Transformation\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse exploratoire :\n",
    "Utiliser des fonctions de calcul sur fenêtre pour : \n",
    "Trouver les produits les plus caloriques par catégorie. \n",
    "Identifier les tendances de production par brands (marques). \n",
    "Générer des statistiques descriptives (e.g., médiane, moyenne des nutriments par catégorie"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, when, avg, max, min, count, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# --- Étape 1 : Catégoriser les produits en fonction du score nutritionnel ---\n",
    "# Ajouter une catégorie de score nutritionnel\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"score_category\",\n",
    "    when(col(\"nutrition_score\") > 0, \"Positif\")\n",
    "    .when(col(\"nutrition_score\") == 0, \"Neutre\")\n",
    "    .otherwise(\"Négatif\")\n",
    ")\n",
    "\n",
    "# Compter les produits par catégorie de score\n",
    "score_category_counts = df_transformed.groupBy(\"score_category\").count()\n",
    "\n",
    "# Afficher le résultat\n",
    "print(\"Distribution des catégories de score nutritionnel :\")\n",
    "score_category_counts.show()\n",
    "\n",
    "# --- Étape 2 : Identifier les produits les plus sains et les moins sains ---\n",
    "# Top 10 des produits les plus sains\n",
    "top_healthy = df_transformed.orderBy(col(\"nutrition_score\").desc()).select(\"product_name\", \"nutrition_score\").limit(10)\n",
    "print(\"Top 10 des produits les plus sains :\")\n",
    "top_healthy.show(truncate=False)\n",
    "\n",
    "# Top 10 des produits les moins sains\n",
    "top_unhealthy = df_transformed.orderBy(col(\"nutrition_score\").asc()).select(\"product_name\", \"nutrition_score\").limit(10)\n",
    "print(\"Top 10 des produits les moins sains :\")\n",
    "top_unhealthy.show(truncate=False)\n",
    "\n",
    "# --- Étape 3 : Produits les plus caloriques par catégorie ---\n",
    "# Définir une fenêtre pour trouver les produits les plus caloriques\n",
    "window_spec = Window.partitionBy(\"categories\").orderBy(col(\"energy-kcal_100g\").desc())\n",
    "\n",
    "# Ajouter une colonne avec le rang des calories\n",
    "df_transformed = df_transformed.withColumn(\"calorie_rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filtrer pour obtenir les produits les plus caloriques par catégorie\n",
    "most_caloric_products = df_transformed.filter(col(\"calorie_rank\") == 1).select(\n",
    "    \"categories\", \"product_name\", \"energy-kcal_100g\"\n",
    ")\n",
    "print(\"Produits les plus caloriques par catégorie :\")\n",
    "most_caloric_products.show(truncate=False)\n",
    "\n",
    "# --- Étape 4 : Analyse des tendances des marques ---\n",
    "# Compter le nombre de produits par marque\n",
    "brand_trends = df_transformed.groupBy(\"brands\").agg(\n",
    "    count(\"*\").alias(\"product_count\")\n",
    ").orderBy(col(\"product_count\").desc())\n",
    "\n",
    "print(\"Tendances de production par marques (Top 10) :\")\n",
    "brand_trends.limit(10).show(truncate=False)\n",
    "\n",
    "# --- Étape 5 : Statistiques descriptives par catégorie ---\n",
    "# Calculer les statistiques descriptives pour chaque catégorie\n",
    "category_statistics = df_transformed.groupBy(\"categories\").agg(\n",
    "    avg(\"energy-kcal_100g\").alias(\"avg_calories\"),\n",
    "    avg(\"fat_100g\").alias(\"avg_fat\"),\n",
    "    avg(\"proteins_100g\").alias(\"avg_proteins\"),\n",
    "    avg(\"sugars_100g\").alias(\"avg_sugars\"),\n",
    "    avg(\"salt_100g\").alias(\"avg_salt\")\n",
    ").orderBy(\"categories\")\n",
    "\n",
    "print(\"Statistiques descriptives par catégorie :\")\n",
    "category_statistics.show(truncate=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sauvegarde des données Save :\n",
    "Partitionner les données par catégories (categories) et années (year). \n",
    "Sauvegarder les résultats transformés en format Parquet avec compression Snappy. \n",
    "Sauvegarder les résultats transformés dans les bases de données: postgresql/sqlserver/mysql/Snowflake/BigQuery"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:38:05.244868Z",
     "start_time": "2024-12-03T13:38:03.841372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Sauvegarde des données (load)\")\n",
    "\n",
    "mysql_url = \"jdbc:mysql://localhost:3306/openfood_db\"\n",
    "mysql_table = \"food_facts_entity\"\n",
    "mysql_user = \"root\"\n",
    "mysql_password = \"root\"\n",
    "\n",
    "df_transformed.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", mysql_url) \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", mysql_table) \\\n",
    "    .option(\"user\", mysql_user) \\\n",
    "    .option(\"password\", mysql_password) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde des données (load)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o17608.save.\n: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 17\u001B[0m\n\u001B[1;32m      5\u001B[0m mysql_user \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroot\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      6\u001B[0m mysql_password \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroot\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[43mdf_transformed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjdbc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43murl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmysql_url\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdriver\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcom.mysql.cj.jdbc.Driver\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdbtable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmysql_table\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmysql_user\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpassword\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmysql_password\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m---> 17\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/ETL_project/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1461\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1459\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m   1460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1461\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1462\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1463\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
      "File \u001B[0;32m~/anaconda3/envs/ETL_project/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/anaconda3/envs/ETL_project/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/anaconda3/envs/ETL_project/lib/python3.10/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o17608.save.\n: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Présentation des résultats :\n",
    "Visualiser les résultats sous forme de graphiques ou tableaux \n",
    "(les étudiants peuvent utiliser un outil comme Jupyter Notebook en local ou Google Colab "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Présentation des données\")"
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6107906,
     "sourceId": 9935398,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
